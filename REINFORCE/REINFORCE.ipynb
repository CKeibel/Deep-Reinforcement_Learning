{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May  9 10:00:36 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 00000000:05:00.0 Off |                    0 |\r\n",
      "| N/A   48C    P0    84W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Auswahl des Spiels**\n",
    "\n",
    "[Hier](https://gym.openai.com/envs/#atari) ist eine vollständige Liste der verfügbaren Spiele zu finden. Um ein Environment zu erstellen muss der vollständige Name des Spiels als String übergeben werden.\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "```python\n",
    "game = \"MsPacman-v0\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier kann das Spiel übergeben werden\n",
    "game = \"Breakout-v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing**\n",
    "[Stable Baselines](https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env) \n",
    "        self.env.reset()\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        observation, _, _, _ = self.env.step(env.unwrapped.get_action_meanings().index('FIRE'))\n",
    "\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # careful! This undoes the memory optimization, use\n",
    "        # with smaller replay buffers only.\n",
    "        return np.array(observation).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic Life Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done  = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
    "            # so it's important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip Reward Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "        return np.sign(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize & Grayscale Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "import cv2\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Warp frames to 84x84 as done in the Nature paper and later work.\n",
    "        :param env: (Gym Environment) the environment\n",
    "        \"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.height, self.width, 1),\n",
    "                                            dtype=env.observation_space.dtype)\n",
    "        \n",
    "    def observation(self, frame):\n",
    "        \"\"\"\n",
    "        returns the current observation from a frame\n",
    "        :param frame: ([int] or [float]) environment frame\n",
    "        :return: ([int] or [float]) the observation\n",
    "        \"\"\"\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "[OPTIONAL]\n",
    "\n",
    "Diese Zelle ist optional ausführbar und dient zur Visualisierung des Wrappers.\n",
    "Die Zelle hat keinen Einfluss auf den Agenten\n",
    "\"\"\"\n",
    "\n",
    "def WarpFrameEnv(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = WarpFrame(env)\n",
    "    return env\n",
    "\n",
    "normal_env = gym.make(game)\n",
    "wrapped_env = WarpFrameEnv(game)\n",
    "\n",
    "normal_env.reset()\n",
    "wrapped_env.reset()\n",
    "action = normal_env.action_space.sample()\n",
    "\n",
    "normal_state, _, _, _ = normal_env.step(action)\n",
    "wrapped_state, _, _, _ = wrapped_env.step(action)\n",
    "\n",
    "wrapped_state = wrapped_state[: , :, 0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.suptitle('Warp Frame', fontsize=20)\n",
    "axs[0].imshow(normal_state)\n",
    "axs[0].set_title(\"Normal\", fontsize=16)\n",
    "axs[1].imshow(wrapped_state, cmap=\"gray\")\n",
    "axs[1].set_title(\"Warp Frame\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame Stack Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.frames = deque(maxlen=4)\n",
    "        low = np.repeat(self.observation_space.low[np.newaxis, ...], repeats=4, axis=0)\n",
    "        high = np.repeat(self.observation_space.high[np.newaxis, ...], repeats=4, axis=0)\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=self.observation_space.dtype)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        frame_stack = np.asarray(self.frames, dtype=np.float32)\n",
    "        frame_stack = np.moveaxis(frame_stack, 0, -1).reshape(1, 84, 84, -1)\n",
    "        return frame_stack, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        for _ in range(4):\n",
    "            self.frames.append(obs)\n",
    "        frame_stack = np.asarray(self.frames, dtype=np.float32)\n",
    "        frame_stack = np.moveaxis(frame_stack, 0, -1).reshape(1, 84, 84, -1)\n",
    "        return frame_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "[OPTIONAL]\n",
    "\n",
    "Diese Zelle ist optional ausführbar und dient zur Visualisierung des Wrappers.\n",
    "Die Zelle hat keinen Einfluss auf den Agenten\n",
    "\"\"\"\n",
    "\n",
    "def FrameStackEnv(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = WarpFrame(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = FrameStack(env)\n",
    "    return env\n",
    "\n",
    "env = FrameStackEnv(game)\n",
    "env.reset()\n",
    "\n",
    "for _ in range(1, 5):\n",
    "  # Führe eine zufällige Aktion aus\n",
    "  state, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# Stack umformen, damit das Plotten der vier Bilder gelingt\n",
    "state = state.reshape(84, 84,4)\n",
    "\n",
    "# Frame Stack plotten\n",
    "fig, axs = plt.subplots(1,4, figsize=(15, 5))\n",
    "fig.suptitle('Frame Stack', fontsize=20)\n",
    "for i in range(state.shape[2]):\n",
    "    axs[i].imshow(state[:, :, i], cmap=\"gray\")\n",
    "    axs[i].set_title(\"Frame \"+str(i+1), fontsize=16)\n",
    "    axs[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellen des Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTS/Breakout-v0/NoEpisodicLife_NoClipReward_PRETRAINED_w_EpisodicLife/\n"
     ]
    }
   ],
   "source": [
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    #env = EpisodicLifeEnv(env)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    env = NoopResetEnv(env)\n",
    "    #env = ClipRewardEnv(env)\n",
    "    env = WarpFrame(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = FrameStack(env)\n",
    "    return env\n",
    "\n",
    "env = make_env(game)\n",
    "\n",
    "\n",
    "\"\"\" saving the properties for csv \"\"\"\n",
    "MODE = \"NoEpisodicLife_NoClipReward_PRETRAINED_w_EpisodicLife\"\n",
    "PATH = \"WEIGHTS/\" + game + \"/\" + MODE + \"/\"\n",
    "print(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Convolutional Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\"\"\"\n",
    "# Tell TF to not use all GPU RAM\n",
    "config = tf.compat.v1.ConfigProto\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = .2\n",
    "session = tf.Session(config=config)\n",
    "\"\"\"\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "# DQN und Tagret Net Parameters\n",
    "INPUT_SHAPE = (84, 84, 4) # (Höhe, Breite, Frames)\n",
    "OUTPUT_SHAPE = env.action_space.n # Anzahl der möglichen Aktionen\n",
    "LOSS_FUNCTION = \"categorical_crossentropy\"\n",
    "OPTIMIZER = RMSprop(learning_rate=0.00025, rho=0.95, epsilon=0.01)\n",
    "\n",
    "# Funktion zum erstellen eines neuronalen Netzes\n",
    "def build_neural_net(INPUT_SHAPE, OUTPUT_SHAPE, LOSS_FUNCTION, OPTIMIZER):\n",
    "    net_input = Input(shape=INPUT_SHAPE)\n",
    "    x = Conv2D(filters=32, kernel_size=(8, 8), strides=(4, 4), padding=\"same\")(net_input)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dense(env.action_space.n, kernel_initializer='he_uniform')(x)\n",
    "    net_output = Activation(\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=net_input, outputs=net_output)\n",
    "    model.compile(loss=LOSS_FUNCTION, optimizer=OPTIMIZER)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Agent Network\n",
    "CNN = build_neural_net(INPUT_SHAPE, OUTPUT_SHAPE, LOSS_FUNCTION, OPTIMIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aktion wählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state):\n",
    "    policy = CNN.predict(state)[0]\n",
    "    action = np.random.choice(env.action_space.n, p=policy)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discount Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(REWARDS):\n",
    "    d_rewards = np.zeros_like(REWARDS)\n",
    "    val = 0.0\n",
    "    \n",
    "    for t in reversed(range(len(REWARDS))):\n",
    "        val = val * GAMMA + REWARDS[t]\n",
    "        d_rewards[t] = val\n",
    "\n",
    "    return d_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize discounted rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_discounted(d_rewards):\n",
    "    d_rewards -= np.mean(d_rewards) # Mittelwert abziehen\n",
    "    d_rewards /= np.std(d_rewards) # durch Standardabweichung teilen\n",
    "    return d_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_sample(state, action, reward):\n",
    "    STATES.append(state)\n",
    "    ACTIONS.append(action)\n",
    "    REWARDS.append(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(STATES, ACTIONS, REWARDS):\n",
    "    episode_length = len(STATES)\n",
    "\n",
    "    d_rewards = discounted_rewards(REWARDS)\n",
    "    d_rewards = normalize_discounted(d_rewards)\n",
    "\n",
    "    states = np.zeros((episode_length, 84, 84, 4))\n",
    "    q_values = np.zeros((episode_length, env.action_space.n))\n",
    "\n",
    "    for i in range(episode_length):\n",
    "        states[i] = STATES[i]\n",
    "        q_values[i][ACTIONS[i]] = d_rewards[i]\n",
    "\n",
    "    CNN.fit(states, q_values, verbose=0)\n",
    "    STATES, ACTIONS, REWARDS = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainings des Agenten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial Mean Reward -> Mean Reward kann nich auf 0 gesetzt werden (Pong -21)\n",
    "INITIAL_MEAN_REWARD = 0.0\n",
    "env.reset()\n",
    "while True:\n",
    "    _, reward, done, _ = env.step(env.action_space.sample())\n",
    "    INITIAL_MEAN_REWARD += reward\n",
    "    if done:\n",
    "        break\n",
    "INITIAL_MEAN_REWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-45ee2caa9103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-89c56c5808b9>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    432\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    119\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    120\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     return [_non_none_constant_value(out)\n\u001b[0;32m---> 84\u001b[0;31m             for out in distributed_function(input_fn)]\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1820\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPISODES = 10_000\n",
    "REWARD_LIST = []\n",
    "MEAN_LIST = []\n",
    "BEST_MEAN_REWARD = INITIAL_MEAN_REWARD\n",
    "STATES, ACTIONS, REWARDS = [], [], []\n",
    "GAMMA = 0.99\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    EPISODE_REWARD = 0.0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Reward einer Aktion zum gesamten Reward der Episode addieren\n",
    "        EPISODE_REWARD += reward\n",
    "        \n",
    "        # Transition zwischenspeichern\n",
    "        append_sample(state, action, reward)\n",
    "        \n",
    "        # State aktualisieren\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            update_policy(STATES, ACTIONS, REWARDS)\n",
    "            \n",
    "            REWARD_LIST.append(EPISODE_REWARD)\n",
    "            current_mean_reward = np.mean(REWARD_LIST[-min(len(REWARD_LIST), 10):])\n",
    "            MEAN_LIST.append(np.mean(REWARD_LIST))\n",
    "            \n",
    "            print(\"Episode:\", episode+1, \"\\tReward:\", EPISODE_REWARD, \"\\tMean:\", round(current_mean_reward, 2),\"\\tBestMean:\", BEST_MEAN_REWARD)\n",
    "\n",
    "            # Übernahme des höchsteb Mean Rewards\n",
    "            if current_mean_reward > BEST_MEAN_REWARD:\n",
    "                BEST_MEAN_REWARD = current_mean_reward\n",
    "\n",
    "                import os\n",
    "                try:\n",
    "                    os.makedirs(PATH)\n",
    "                except FileExistsError:\n",
    "                    # Pfad existiert bereits\n",
    "                    pass\n",
    "                CNN.save_weights(PATH +\"Best.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
