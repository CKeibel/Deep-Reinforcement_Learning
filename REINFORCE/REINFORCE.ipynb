{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 24 09:39:38 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 00000000:05:00.0 Off |                    0 |\r\n",
      "| N/A   47C    P0    74W / 149W |  11034MiB / 11441MiB |      6%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Auswahl des Spiels**\n",
    "\n",
    "[Hier](https://gym.openai.com/envs/#atari) ist eine vollständige Liste der verfügbaren Spiele zu finden. Um ein Environment zu erstellen muss der vollständige Name des Spiels als String übergeben werden.\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "```python\n",
    "game = \"MsPacman-v0\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier kann das Spiel übergeben werden\n",
    "game = \"Pong-v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing**\n",
    "[Stable Baselines](https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env) \n",
    "        self.env.reset()\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        observation, _, _, _ = self.env.step(env.unwrapped.get_action_meanings().index('FIRE'))\n",
    "\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # careful! This undoes the memory optimization, use\n",
    "        # with smaller replay buffers only.\n",
    "        return np.array(observation).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic Life Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done  = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
    "            # so it's important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip Reward Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "        return np.sign(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize & Grayscale Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "import cv2\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Warp frames to 84x84 as done in the Nature paper and later work.\n",
    "        :param env: (Gym Environment) the environment\n",
    "        \"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.height, self.width, 1),\n",
    "                                            dtype=env.observation_space.dtype)\n",
    "        \n",
    "    def observation(self, frame):\n",
    "        \"\"\"\n",
    "        returns the current observation from a frame\n",
    "        :param frame: ([int] or [float]) environment frame\n",
    "        :return: ([int] or [float]) the observation\n",
    "        \"\"\"\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEMCAYAAAA/Jfb8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3debhcVZnv8e/PhBDmhMEYCUjAiALaASPDRSIS0Mgg2LRIWhFQb/A22NDYzaBcoRUftRVou1tiB0GwReZB2qbBEObbEkkCMiMQgiSGBEjCFCAceO8faxXs1Kk6U1WdqrP5fZ6nnqpae3rPJry1atXe61VEYGZm5fKOdgdgZmbN5+RuZlZCTu5mZiXk5G5mVkJO7mZmJeTkbmZWQk7uZmYl5ORu3Uj6sKSQNKfO8ml5eUgaX2P5OpJekbRK0tqtj7hxkr5S+JtqPbraHaNZfwxvdwDWke4CVgAflrRhRDxftXwKEICAvYBzq5bvDqwNzIqIV1sdbJPdBVxTo/2NwQ7ErBFO7tZNRLwh6WbgM8DHgP+sWmUv4GbgQ9RO7nvl59mti7Jl5kfEae0OwqxRHpaxeiqJea9io6StgPF5+S3Ax2ts2y25Sxol6QRJN0laLGm1pGWSrpa0S/UOJA3PwyE3SHq3pPMk/VnS65K+kNf5ZV5nS0n/IOnhPBz0pKQzJG3Q8FmoQdLIfNzrJG0u6XxJS3Jsh+Z1PiDpnyTNk/SMpFclPS5phqSxNfY5Ne/zJEm7Spol6XlJyyVdIundeb33Sbos73NVPj/b14lzPUmnSLpH0kuSXpB0u6S/asV5sc7i5G713Jifp1S1TyksvwkYK2m7ykJJGwKTSMM68wvb7QCcDnSRvgmcSUr++wC3Sdq7ThybAncAHwGuAH4CLKta51+Bk3M8P87HPh64ocVj/u8E5gA7ApcBZwPP5GWfA74CLAQuBP4NeAQ4CrhT0pg6+9yd9K1oNTCTNEx0CPDbnMTn5ONeAFxP+u8xS9I6xZ1I2oR03r4DvEr6dvUfwObAZZK+2dBfbp0vIvzwo+YD+DNprHmzQtuFwAukIb3tSWPvxxSWH5Dbrqza1yhgkxrHeA/wFHBvVfvwvJ8Afg4Mq7HtL/PyZcAWhfZhwNV52cl9/Fu/ktefD5xW4/GhwrojC7GdUye2ccCIGu0H5HN6VlX71MI+D67zdy4Hvl617Lt52VFV7Rfn9mOr2tchfTC/Dnyg3f/G/Gjdo+0B+NG5D1JPL4BDCm1/Bq4tvF9aTOTAWXmbo/txnLPzNu8utFWS+8u1PhTyOpWk1y2BAxNyEn2kjzFUknu9xxcK61aS+ypg9ADO6x+BB6raKsl9Vo31P5GXPQSoatm2edmMQtvYnLxvq3P8XfI23273vzE/WvfwD6rWkxuBL5DG0C+V9AFS4jirsM7NwD6S3hERb9DDj6mS9gD+FtiVNLQwomqVzUkfHkULIuLZXuK8pbohIh6R9GfgvZI2iIgXetlHxbkR8ZU+rvtIRKyotUCSgMOBL5J+eB5F+kZRUX0FUsXcGm2Vc3JX5OxcsDg/jyu07UIach0m6bQa+6sM4XygTgxWAk7u1pNKgp5S9XxjYZ2bSWPCO0r6E/BBYHFEPFTckaTPkoYKXgZmAQuAl0i9672APUiXT1Z7qg9xLq3T/hTpA2ND0lBSs/UU29nAV0nJ97/z8yt52VdIvyXU8lyNtq4+LFur0LZJft4tP+pZv4dlNsQ5uVtdEfEnSY+Rer9bkJLwStKPfBU35ee9gCdI177XugTyO6Tk9uGIeLi4IO97j3ph9CHUMcBjNdrflZ/r9ZIbVTO2/Pd8lTR+v0dErKpafmSL4qmofAh8LyK+0eJjWYfy1TLWm0qi3hvYE7glD78AkHvoT5GSe0/Xt28D3FcjsQ8jXSHSiI9VN0iaALwbeLQfQzLNsk1+vq5GYh8PbNHi488hffDU+8C0twEnd+tNZQjm74DRvNVTL7qJlEg+kd/XSu5PANtKqvSmK+PS3yb9KNiIv8u95cp+hwE/JH2L+HmD+x6Ihfl5sqQ3/x/Ll4mek+NqmYh4Ergc+Gi+t2BY9TqSJkjaspVxWHt5WMZ6cyOpF/jBwvtqNwHTSDc3PRwRi2uscxbpWu+7JV1BGiveA3gf8Btg/wZi/B3wB0mXkoYkPpXjvRM4o4H9DkhELJR0NXAQMF/SDaQPxk+ShrXuB97b4jCOArYGfgB8SdLtpGvw3w1sB3yYdAfyn1och7WJe+7Wo4h4Grg3v30GuK/GasXefM0pByLiJ8CXST9+Hgl8ntTD3QX4Q4Nhfg34Hulu2eOAjUkfJlOifXPbHAb8E7ABcAxpWOtK4KPAi60+eL6KZ3fSN66VpB+9jyMNYa0AjqXGVUZWHup+ZZXZ0CDpl6QPiS0iYlG74zHrJO65m5mVkJO7mVkJObmbmZWQx9zNzErIPfcmkXREno97paTRVcsqc5Of1qbwBqTwN23V7ljMrH+c3JtvI+DEdgdhZm9vTu7N91vgaz0UY2hIi4tPmFlJOLk33+n5+ZSeVpK0cy6R9mIugTZb0s5V65wvaZGk3ST9j6SXSTfGIGlhLjN3WC4v97Kk2/Jt5etJ+ndJz0pamkvODS/sd6SksyTdl4//lKT/lPT+Zp8MM2sPJ/fmW0K6zX66pPfUWkHSh0h3B44GjiDN+b0hcIukv6hafSPSVLkXkW6r/1Vh2WTgb0jDQIeTJqy6greqJR1KKtV2PDC9sN3apDsnTwf2A/4PqQDF74pzv5jZ0OW5ZVrjB6S5PU4FvlRj+bdIdS2nRMRKAEmzSLfjnwr8ZWHd9UlVgH5dYz/rA1Mj4rm8j3eRaoj+PiL+Pq8zS9J+wGdJc4yT13+zIEWeWOp60tQA01izGIeZDUHuubdARCwnTVj1RUm1ZjycDPymktjzNs8D19B9+trXSBNr1fK7SmLPKgUyrq9a7yGqppmVdIikOZJWkibxeon0YdHoDI1m1gGc3FvnLFJB42/XWLYxafim2lOkoZqipyPi9TrHqC7xtrqH9pGVN5IOAC4BHgT+mjR510eAp4vrmdnQ5WGZFomIFyV9j9SD/2HV4uW8VSWo6F10T8ytuMvsUFIRiyMqDZLWIn3omFkJuOfeWmeTameeXtV+C7CvpA0qDfn1AaSapK22Lm/V3qw4jDULOJvZEObk3kJ5LvFvk4o0FH2HlGBnSzpY0l8CN+S2WsM4zXYd8P58OeQUSSfm467sZTszGyKc3Fvv58AjxYaIuIdUj/R54ALgP0gFHD4WEY0WruiLc4DvAp8D/hPYl/St4bmeNjKzocMTh5mZlZB77mZmJdSy5C5par4t/lFJJ7XqOGZm1l1LhmXyHY9/BPYBFpGq0E+LiAeafjAzM+umVT33nUnXUS+IiNWkuVEObNGxzMysSqtuYtoceLLwfhHpLsiaJPX49WGLDX35tTXmyedffyYiNmt3HGaDpW13qEqaTp6pcPTId3Dqnhu1K5Q37fO/duvX+rP+53ctimTomHv8fn1ed9KZ/9XCSHp23HUrnmjbwc3aoFXDMotZc6KqcbntTRExMyImRcSk9UeoRWGYmb09tSq53wlMkDRe0gjSXCbXtOhYZmZWpSXDMhHRJekY0tSzw4DzIuL+VhzLzMy6a9mYe0RcC1zbqv0Phuox9f6Oyb8dVY+r92dM3syax3eompmVkJO7WQlImiYpJE2uah+T25fW2ObovGyHwYu0Pkmn53hqPbZqd3xDjYt1mJXDrfl5cuF15f0q4J2S3h8RD1UtexbotN/Dao1/1qpcZj1wcjcrgYhYLOkxUsIumgzcCHwgvy4m9z2A26MJc5BIWjvXL2hYRNzRrmOXiYdlzMrjVmA3ScVO22TgNuB2Colf0gRgLKkqWKVtF0lXSFok6eU88d/pktaoqyvpdkk3SzpI0t2SXgWmSxqeh1D+UdL/lbRY0iuSbpH0wWb8gZK+mo+xm6SrJD1X+RsKbZX4H8qxrF21jzsk3SDpAEn35HXnSvqwpLUk/VDSUknPSjpH0jpV228g6QxJT0haLekxSSdI6qgbdtxzNyuPW4EjgZ2A30saBexASu7PAt8qrDu5sE3Fe4D5pAIzLwLb5222Ar5QdawPAGeSKngtzPuv+FJuOxpYJ69zo6QJEdFrta+qDyeANyLijaq2i4ELgX/jrfKQW5HusTk3x//BHP97gCOqtt+OVBHtdOAV4EfAr0kV0VYDXwQ+BHyPNCT0rRzbiLzO+Lz9g8DueT8bAd/s7e8bLE7uZuVR6YVPBn5PGnZ5FZhHSr5bStoqIhbmdZ4H7q5sHBGXVl7nXujtwEvAuZKOqUrMmwF7R8S9hW0q+WRt4JMRsSq3/x54GDgW+Mc+/B2vVb2/gO7J+VcR8Y1iQ0RcVCP+l4GfSvpaRLxQWH1jYJeIeDKvPxK4BNg0IvbP61wv6ePAZ3nrg/Fw4CPAbhExJ7fdkP/2v5f0w758gA0GD8uYlUREPE6apK/SK58MzImI1RHxR2BZ1bL/FxGvV7aXNCoPSSwgfSi8RurFvwN4b9XhHi0m9iq/qST2HNdjpB51X28U+UjV47Qa61xV3SBpdB4uKcZ/Dqlnv03V6vdXEntW+S3i+qr1HmLNqVSmkqYzn5eHoYbnxP5bYCRpRtyO4J67WbncCnwq91wns2ayuh2YLOlG0hDGv1dtewHwMVIv9Q+kXvtuwL+QEldRT1evdLvsMrdVJ9iaImJuH1ardfxfkuI9lRT/KtK3lzPpHv+Kqvere2gvbvtOYFu6f7uo2KTHqAeRk7tZudwC/DWwK2ns/ZTCstuAvyElcCiMt0taD9gf+GZE/Euhfcc6x+npCpsxddoW12gfqDWOL2lD4FPACRHxr4X2jzTxmJCGtx6m+28QFQuafLwBc3Lvgacb6D9PN9B2lYR9EiCgOIfG7cBZwCGkXu2dhWUjScMvb/ZIc+//iAHEsL+kdQtj7tuQhle+M4B99dU6pL+3Ov7Dm3yc60hDMyvycFPHcnI3K5GIeEjSMuAAYF5EvFhYfBfpKpIDgJsi4rXCds9KmguckO9mXQF8hdq98N68Svox8kekpPudvL8fD+Rv6ouIWCrpbuAkSc8AK0n1IjZt8qF+TvrAuEnSGcB9pB+Q3wt8mvRD8us9bD9o/IOqWfncSurF3lZszEnnd3nZrTW2+xzpA2AGKYk9CRw/gOOfR/qB8WzgfODPwJRBuIrks8C9pN8SzgMeB/6hmQfIN0tNAX5ButTzWuA/gM+Tzmn1JZtt05IC2f215UbD4+v/a8N2h+FKTAMwhCoxzYuISW0L4G0gXzXyGvCPEXFam8N52/OwTIGTdf+1M2GbWX0DHpaRtIWkmyQ9IOl+Scfm9tPybcd358e+zQvXzMz6opGeexfw9YiYL2kD0kX9s/KysyLiR42HZ9ZZJE0l/TA4DPhZRHy/zSF1jIjoIo3nWwcYcHKPiCXkGwki4gVJDwKbD2RfG4/fgS/8cvZAQzHr1XGbNn7RhKRhwE+AfUh3gt4p6ZqIeKDhnZs1WVPG3PNE+jsCc0iT6Bwj6YvAXFLvvvquLyRNJ12qxLhx45oRhlmr7Uy67X4BgKSLgQOBusldUt0rFtZeO01W+I53tPaitcpkheuuu26ft3n55Zd5442OufBjUFXO1zrrrNPLmm955ZVXAAb9nK1evZqurq6a35YaTu6S1geuAI6LiOclzSBd1xr5+QzSLHFriIiZwEyAiRMntv+SHbPebU66PLBiEbBL9UrFjstaa63F9ttvPzjR1TFyZLp7/oADDujzNjfccAMrVnTrk70tVM7Xfvv1/UqwG2+8EWDQz9nDDz9cd1lDXQZJa5ES+4URcSWkmwki4vU8Rec5dNBEOmaDISJmRsSkiJg0fLgvSLP2GPC/vHxr77nAgxFxZqF9bB6PB/gM6Q4uszJYzJozBI6jufOlDJrLLrtsjfeVXn2l12rdXXHFFW++rvTqO/l8NdKt2B04DLg33/YL8A1gmqSJpGGZhcBRDUVo1jnuBCZIGk9K6oeSJuky6ziNXC1zO7Uve7p24OGYda6I6JJ0DGka3WHAeRHRacWlzQDfoWrWLxFxLe7A2BDgicPMzEqoI3ruyx+/j19+YUK7wzAzKw333M3MSqgjeu5mNrhGjx69xvvKXZlWX/Gctfqu4mbo/AjNzKzf3HM3exvae++92x3CkLPXXnu1O4R+cc/dzKyE3HM3K7nXXkt1sOfOndvnbVatWtWqcDpe5XzNnz+/z9t04vlycjdrodGjR3PwwQe3O4x+22mnndodwpDSrvM1Y8aMuss8LGNmVkLuuZu10NixYznllFPaHYaV1NVXX113mXvuZmYl5ORuZlZCTu5mZiXUjBqqC4EXgNeBroiYJGlj4BJgK1LBjkNqFck2M7PWaFbP/eMRMTEiJuX3JwGzI2ICMDu/NzOzQdKqq2UOBPbMry8AbgZObNGxzDrWihUr1qi9adZMK1bUHxBpRnIP4LeSAvj3iJgJjCkUyX4KGFO9kaTpwHSA0SM99G/ltGTJEk4//fR2h2EltWTJkrrLmpHcPxoRiyW9E5gl6aHiwoiInPipap8JzATYcqPh3ZabmdnANdxljojF+XkZcBWwM7BU0liA/Lys0eOYmVnfNZTcJa0naYPKa+ATwH3ANcDhebXDgV83chwzM+ufRodlxgBX5Souw4FfRcR1ku4ELpX0ZeAJ4JAGj2NmZv3QUHKPiAXAX9RofxaY0si+zcxs4HyZiplZCTm5m5mVkJO7mVkJObmbmZWQk7tZFUlbSLpJ0gOS7pd0bG7fWNIsSY/k59HtjtWsHid3s+66gK9HxHbArsDRkrbDE+LZEOLkblYlIpZExPz8+gXgQWBz0oR4F+TVLgAOak+EZr1zDVWzHkjaCtgRmEMfJsTL27w5Kd5aa63V+iDNanDP3awOSesDVwDHRcTzxWUREaQZUbuJiJkRMSkiJg0f7v6TtYeTu1kNktYiJfYLI+LK3OwJ8WzIcHI3q6I0WdK5wIMRcWZhkSfEsyHD3xnNutsdOAy4V9Ldue0bwPfxhHg2RDi5m1WJiNsB1VnsCfFsSPCwjJlZCTm5m5mV0ICHZSRtC1xSaNoa+BYwCvjfwNO5/RsRce2AIzQzs34bcHKPiIeBiQCShgGLSTVUjwTOiogfNSVCMzPrt2YNy0wBHouIJ5q0PzMza0CzkvuhwEWF98dIukfSefVmzpM0XdJcSXNfXF3zRj8zMxughpO7pBHAp4HLctMMYBvSkM0S4Ixa2xVv0V5/RL2rzszMbCCa0XP/FDA/IpYCRMTSiHg9It4AzgF2bsIxzMysH5qR3KdRGJKpzL2RfQa4rwnHMDOzfmjoDlVJ6wH7AEcVmv9J0kTSjHkLq5aZmdkgaCi5R8RLwCZVbYc1FJGZmTXMd6iamZWQk7uZWQk5uZuZlZCTu5lZCTm5m5mVkJO7mVkJObmbmZWQk7uZWQk5uZuZlZCTu5lZCTm5m5mVkJO7mVkJObmbmZVQQ7NCmrXS3OP3W+P9pDP/q02RmA09fUruks4D9geWRcQOuW1j4BJgK9K87YdExApJAn4M7AusAo6IiPnND93MbOgZOzbVM3rXu94FwPLly3niiSeafpy+DsucD0ytajsJmB0RE4DZ+T2ksnsT8mM6qaaqmZkNoj4l94i4FVhe1XwgcEF+fQFwUKH9F5HcAYyqKr1nNiRIGibpLkm/ye/HS5oj6VFJl+Ti8Gb9MmrUKEaNGsXWW2/N1ltvzcYbb9yS4zTyg+qYiFiSXz8FjMmvNweeLKy3KLeZDTXHAg8W3v8AOCsi3gusAL7clqjM+qApV8tERJBqpvaZpOmS5kqa++Lqfm1q1nKSxgH7AT/L7wXsBVyeVyl+WzXrOI0k96WV4Zb8vCy3Lwa2KKw3LretISJmRsSkiJi0/gg1EIZZS/wzcALwRn6/CbAyIrry+7rfSIsdl66urlqrmLVcI8n9GuDw/Ppw4NeF9i8q2RV4rjB8Y9bxJFWuDJs3kO2LHZfhw321sbVHXy+FvAjYE9hU0iLgVOD7wKWSvgw8ARySV7+WdBnko6RLIY9scsxmrbY78GlJ+wIjgQ1Jl/eOkjQ8995rfiM16xR9Su4RMa3Ooik11g3g6EaCMmuniDgZOBlA0p7A30fE5yVdBvwVcDFrfls16ziefsCs704Ejpf0KGkM/tw2x2NWlwcEzXoQETcDN+fXC4Cd2xmPWV85uZuZDaJXXnkFgJUrV67xvtk8LGNmVkLuuZuZDaLHH398jedWcc/dzKyE3HO3juX5280Gzj13M7MScnI3MyshJ3czsxJycjczKyEndzOzEnJyNzMrISd3M7MScnI3MyuhXpO7pPMkLZN0X6Hth5IeknSPpKskjcrtW0l6WdLd+fHTVgZvZma19aXnfj4wtaptFrBDRHwI+CO5sEH2WERMzI+vNidMMzPrj16Te0TcCiyvavttoVDwHaSSY2Zm1iGaMeb+JeC/C+/HS7pL0i2S9qi3UbFC/IurowlhmJlZRUMTh0n6JtAFXJiblgBbRsSzkj4MXC1p+4h4vnrbiJgJzATYcqPhzu5mZk004J67pCOA/YHP56LYRMSrEfFsfj0PeAx4XxPiNDOzfhhQcpc0FTgB+HRErCq0byZpWH69NTABWNCMQM3MrO96HZaRdBGwJ7CppEXAqaSrY9YGZkkCuCNfGTMZ+Lak14A3gK9GxPKaOzYzs5bpNblHxLQazefWWfcK4IpGgzIzs8b4DlUzsxJycjczKyEndzOzEnJyNzMrISd3M7MScnI3q0HSKEmX59lPH5S0m6SNJc2S9Eh+Ht3uOM3qcXI3q+3HwHUR8X7gL4AHgZOA2RExAZid35t1JCd3syqSNiLdkHcuQESsjoiVwIHABXm1C4CD2hOhWe+c3M26Gw88Dfw8z3D6M0nrAWMiYkle5ylgTK2NizOednV11VrFrOWc3M26Gw7sBMyIiB2Bl6gagsmT5dWczTQiZkbEpIiYNHx4QxOvmg2Yk7tZd4uARRExJ7+/nJTsl0oaC5Cfl7UpPrNeObmbVYmIp4AnJW2bm6YADwDXAIfntsOBX7chPLM+8XdGs9q+BlwoaQRp2uojSZ2hSyV9GXgCOKSN8Zn1yMndrIaIuBuYVGPRlMGOxWwgPCxjZlZCvSZ3SedJWibpvkLbaZIWS7o7P/YtLDtZ0qOSHpb0yVYFbmZm9fWl534+MLVG+1kRMTE/rgWQtB1wKLB93ubsStk9MzMbPL0m94i4FehrqbwDgYtzoezHgUeBnRuIz8zMBqCRMfdjJN2Th20qEyhtDjxZWGdRbuumeBffi6tr3gtiZmYDNNDkPgPYBpgILAHO6O8OinfxrT9CAwzDzMxqGVByj4ilEfF6RLwBnMNbQy+LgS0Kq47LbWZmNogGlNwrt2BnnwEqV9JcAxwqaW1J44EJwO8bC9HMzPqr15uYJF0E7AlsKmkRcCqwp6SJpImTFgJHAUTE/ZIuJd2q3QUcHRGvtyZ0MzOrp9fkHhHTajSf28P63wW+20hQZmbWGN+hamZWQk7uZmYl5ORuZlZCTu5mZiXk5G5mVkJO7mZmJeTkbmZWQk7uZmYl5ORuZlZCTu5mZiXk5G5mVkJO7mZmJeTkbmZWQk7uZmYl1OuUv2Y2cKNHj+bggw9udxhWUjNmzKi7rNeeey6AvUzSfYW2SyTdnR8LJd2d27eS9HJh2U+b8heYmVm/9KXnfj7wb8AvKg0R8bnKa0lnAM8V1n8sIiY2K0CzoWzs2LGccsop7Q7DSurqq6+uu6zXnntE3Aosr7VMkoBDgIsGGpxZJ5L0d5Lul3SfpIskjZQ0XtIcSY/mb68j2h2nWT2N/qC6B7A0Ih4ptI2XdJekWyTtUW9DSdMlzZU098XV0WAYZs0jaXPgb4FJEbEDMAw4FPgBcFZEvBdYAXy5fVGa9azR5D6NNXvtS4AtI2JH4HjgV5I2rLVhRMyMiEkRMWn9EWowDLOmGw6sI2k4sC7p3/ZewOV5+QXAQW2KzaxXA07u+R/9XwKXVNoi4tWIeDa/ngc8Bryv0SDNBlNELAZ+BPyJlNSfA+YBKyOiK6+2CNi8PRGa9a6RnvvewEMRsajSIGkzScPy662BCcCCxkI0G1ySRgMHAuOBdwPrAVP7sf2bQ45PP/10i6I061lfLoW8CPgdsK2kRZIq44yH0v2H1MnAPfnSyMuBr0ZEzR9jzTrY3sDjEfF0RLwGXAnsDozK31gBxgGLa21cHHLcbLPNBidisyq9XgoZEdPqtB9Ro+0K4IrGwzJrqz8Bu0paF3gZmALMBW4C/gq4GDgc+HXbIjTrhacfMKsSEXNI3zznA/eS/j+ZCZwIHC/pUWAT4Ny2BWnWC08/YFZDRJwKnFrVvADYuQ3hmPWbe+5mZiXknrtZC82bN+8ZSS8Bz7Q7loJNcTy96bSY6sXznnobOLmbtVBEbCZpbkRMancsFY6nd50W00Di8bCMmVkJObmbmZWQk7tZ681sdwBVHE/vOi2mfsfj5G7WYhHRUYnC8fSu02IaSDxO7mZmJeTkbmZWQk7uZi0iaaqkh3PlppPaFMMWkm6S9ECuLHVsbt9Y0ixJj+Tn0YMc17Bc1Oc3+X3bqlxJGiXpckkPSXpQ0m7tPD/NqgKmiPZXQZo4cWLMnj273WFYiW266abzBvO65Tz19R+BfUhzv98JTIuIBwYrhhzHWGBsRMyXtAFpXvqDgCOA5RHx/fzBMzoiThzEuI4HJgEbRsT+ki4FroyIiyX9FPhDRMwYpFguAG6LiJ/lpLku8A3acH5yFbDbge0i4uV8Xq4F9qWf58c9d7PW2Bl4NCIWRMRq0kySBw52EBGxJCLm59cvAA+SiowcSKomBYNcVUrSOGA/4Gf5vWhTlStJG5GmKj8XICJWR8RK2nh+aFIVMCd3s9bYHHiy8L7tlZskbQXsCMwBxkTEkrzoKWDMIIbyz8AJwBv5/Sa0r8rVeOBp4Od5mOhnktajTeenmVXA+lKso19jdkr+JY8N3SNpp4H9mWbWLJLWJ9VaOC4ini8uizQ2Oyjjs5L2B5blMpydYDiwEzAj135+CVjj95FBPj8NVXpI9jYAAARLSURBVAEr6kvPvQv4ekRsB+wKHC1pO9IJmB0RE4DZvHVCPkUqrzcBmA4MyriZWYdZDGxReF+3clOrSVqLlNgvjIgrc/PSPB5fGZdfNkjh7A58WtJC0lDVXsCP6WOVqxZYBCzKc/hDGvrYifadn4aqgBX1mtwHMGZ3IPCLSO7IQY3t859mVg53AhPyVQ4jSGUprxnsIPJ49rnAgxFxZmHRNaRqUjCIVaUi4uSIGBcRW5HOyY0R8XneqnI12PE8BTwpadvcNAV4gDadHwpVwPJ/u0o8/T4//ZoVso9jdvXGGpcU2pA0ndSzZ9y4cf0Jw6zjRUSXpGOA64FhwHkRcX8bQtkdOAy4N9c2hnQlyPeBS3NN5CeAQ9oQW9GJwMWSTgfuYnCrXH0NuDB/CC8AjiR1fAf9/ETEHEmVKmBdpHMxE/gv+nl++nwpZB6zuwX4bkRcKWllRIwqLF8REaPzdavfj4jbc/ts4MSImFtv374U0lptsC+FNGu3Pl0t088xu44ZazQze7vqy9Uy/R2zuwb4Yr5qZlfgucLwjZmZDYK+jLn3d8yucjfVo8Aq0viVmZkNol6Tex47V53FU2qsH8DRDcZlZmYN8B2qZmYl5ORuZlZCTu5mZiXUEVP+SnqaNKfDM+2OZYA2ZejGDkM7/r7G/p6I2KzVwZh1io5I7gCS5g7Vm0yGcuwwtOMfyrGbtZKHZczMSsjJ3cyshDopuc9sdwANGMqxw9COfyjHbtYyHTPmbmZmzdNJPXczM2sSJ3czsxJqe3KXNFXSw7nm6km9b9F+khZKulfS3ZLm5raaNWU7gaTzJC2TdF+hbUjUwK0T+2mSFufzf7ekfQvLTs6xPyzpk+2J2qz92prcJQ0DfkKqu7odMC3XZx0KPh4REwvXWNerKdsJzqd7kd2hUgP3fGoXCD4rn/+JEXEtQP63cyiwfd7m7PxvzOxtp909952BRyNiQUSsJhXMPbDNMQ1UvZqybRcRtwLLq5qHRA3cOrHXcyBwcUS8GhGPk6ad3rllwZl1sHYn93r1VjtdAL+VNC/XgoX6NWU7VX9r4HaaY/Kw0XmFIbChErtZy7U7uQ9VH42InUhDGEdLmlxcmOe0HzLXmA61eElDRdsAE0mF189obzhmnafdyX1I1luNiMX5eRlwFemrf72asp1qyNbAjYilEfF6RLwBnMNbQy8dH7vZYGl3cr8TmCBpvKQRpB/DrmlzTD2StJ6kDSqvgU8A91G/pmynGrI1cKt+A/gM6fxDiv1QSWtLGk/6Ufj3gx2fWSfoSw3VlomILknHANcDw4DzIuL+dsbUB2OAq1LdcIYDv4qI6yTdSe2asm0n6SJgT2BTSYuAUxkiNXDrxL6npImkoaSFwFEAEXG/pEuBB4Au4OiIeL0dcZu1m6cfMDMroXYPy5iZWQs4uZuZlZCTu5lZCTm5m5mVkJO7mVkJObmbmZWQk7uZWQn9f8oTCP4HhHsuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "[OPTIONAL]\n",
    "\n",
    "Diese Zelle ist optional ausführbar und dient zur Visualisierung des Wrappers.\n",
    "Die Zelle hat keinen Einfluss auf den Agenten\n",
    "\"\"\"\n",
    "\n",
    "def WarpFrameEnv(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = WarpFrame(env)\n",
    "    return env\n",
    "\n",
    "normal_env = gym.make(game)\n",
    "wrapped_env = WarpFrameEnv(game)\n",
    "\n",
    "normal_env.reset()\n",
    "wrapped_env.reset()\n",
    "action = normal_env.action_space.sample()\n",
    "\n",
    "normal_state, _, _, _ = normal_env.step(action)\n",
    "wrapped_state, _, _, _ = wrapped_env.step(action)\n",
    "\n",
    "wrapped_state = wrapped_state[: , :, 0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.suptitle('Warp Frame', fontsize=20)\n",
    "axs[0].imshow(normal_state)\n",
    "axs[0].set_title(\"Normal\", fontsize=16)\n",
    "axs[1].imshow(wrapped_state, cmap=\"gray\")\n",
    "axs[1].set_title(\"Warp Frame\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame Stack Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.frames = deque(maxlen=4)\n",
    "        low = np.repeat(self.observation_space.low[np.newaxis, ...], repeats=4, axis=0)\n",
    "        high = np.repeat(self.observation_space.high[np.newaxis, ...], repeats=4, axis=0)\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=self.observation_space.dtype)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        frame_stack = np.asarray(self.frames, dtype=np.float32)\n",
    "        frame_stack = np.moveaxis(frame_stack, 0, -1).reshape(1, 84, 84, -1)\n",
    "        return frame_stack, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        for _ in range(4):\n",
    "            self.frames.append(obs)\n",
    "        frame_stack = np.asarray(self.frames, dtype=np.float32)\n",
    "        frame_stack = np.moveaxis(frame_stack, 0, -1).reshape(1, 84, 84, -1)\n",
    "        return frame_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "[OPTIONAL]\n",
    "\n",
    "Diese Zelle ist optional ausführbar und dient zur Visualisierung des Wrappers.\n",
    "Die Zelle hat keinen Einfluss auf den Agenten\n",
    "\"\"\"\n",
    "\n",
    "def FrameStackEnv(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = WarpFrame(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = FrameStack(env)\n",
    "    return env\n",
    "\n",
    "env = FrameStackEnv(game)\n",
    "env.reset()\n",
    "\n",
    "for _ in range(1, 5):\n",
    "  # Führe eine zufällige Aktion aus\n",
    "  state, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# Stack umformen, damit das Plotten der vier Bilder gelingt\n",
    "state = state.reshape(84, 84,4)\n",
    "\n",
    "# Frame Stack plotten\n",
    "fig, axs = plt.subplots(1,4, figsize=(15, 5))\n",
    "fig.suptitle('Frame Stack', fontsize=20)\n",
    "for i in range(state.shape[2]):\n",
    "    axs[i].imshow(state[:, :, i], cmap=\"gray\")\n",
    "    axs[i].set_title(\"Frame \"+str(i+1), fontsize=16)\n",
    "    axs[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellen des Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTS/Pong-v0/NoEpisodicLife_NoClipReward_PRETRAINED_w_EpisodicLife/\n"
     ]
    }
   ],
   "source": [
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    #env = EpisodicLifeEnv(env)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    env = NoopResetEnv(env)\n",
    "    #env = ClipRewardEnv(env)\n",
    "    env = WarpFrame(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = FrameStack(env)\n",
    "    return env\n",
    "\n",
    "env = make_env(game)\n",
    "\n",
    "\n",
    "\"\"\" saving the properties for csv \"\"\"\n",
    "MODE = \"NoEpisodicLife_NoClipReward_PRETRAINED_w_EpisodicLife\"\n",
    "PATH = \"WEIGHTS/\" + game + \"/\" + MODE + \"/\"\n",
    "print(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Convolutional Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\"\"\"\n",
    "# Tell TF to not use all GPU RAM\n",
    "config = tf.compat.v1.ConfigProto\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = .2\n",
    "session = tf.Session(config=config)\n",
    "\"\"\"\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "# DQN und Tagret Net Parameters\n",
    "INPUT_SHAPE = (84, 84, 4) # (Höhe, Breite, Frames)\n",
    "OUTPUT_SHAPE = env.action_space.n # Anzahl der möglichen Aktionen\n",
    "LOSS_FUNCTION = \"categorical_crossentropy\"\n",
    "OPTIMIZER = RMSprop(learning_rate=0.00025, rho=0.95, epsilon=0.01)\n",
    "\n",
    "# Funktion zum erstellen eines neuronalen Netzes\n",
    "def build_neural_net(INPUT_SHAPE, OUTPUT_SHAPE, LOSS_FUNCTION, OPTIMIZER):\n",
    "    net_input = Input(shape=INPUT_SHAPE)\n",
    "    x = Conv2D(filters=32, kernel_size=(8, 8), strides=(4, 4), padding=\"same\")(net_input)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dense(env.action_space.n, kernel_initializer='he_uniform')(x)\n",
    "    net_output = Activation(\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=net_input, outputs=net_output)\n",
    "    model.compile(loss=LOSS_FUNCTION, optimizer=OPTIMIZER)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Agent Network\n",
    "CNN = build_neural_net(INPUT_SHAPE, OUTPUT_SHAPE, LOSS_FUNCTION, OPTIMIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aktion wählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state):\n",
    "    policy = CNN.predict(state)[0]\n",
    "    action = np.random.choice(env.action_space.n, p=policy)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discount Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(REWARDS):\n",
    "    gamma = 0.99\n",
    "    running_add = 0\n",
    "    discounted_r = np.zeros_like(REWARDS)\n",
    "    for i in reversed(range(0, len(REWARDS))):\n",
    "        if REWARDS[i] != 0:\n",
    "            running_add = 0\n",
    "        running_add = running_add * gamma + REWARDS[i]\n",
    "        discounted_r[i] = running_add\n",
    "        \n",
    "        discounted_r -= np.mean(discounted_r)\n",
    "        discounted_r /= np.std(discounted_r)\n",
    "\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_sample(state, action, reward):\n",
    "    STATES.append(state)\n",
    "    action_onehot = np.zeros([env.action_space.n])\n",
    "    action_onehot[action] = 1\n",
    "    ACTIONS.append(action_onehot)\n",
    "    REWARDS.append(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(STATES, ACTIONS, REWARDS):\n",
    "    states = np.vstack(STATES)\n",
    "    actions = np.vstack(ACTIONS)\n",
    "    \n",
    "    # discount rewards\n",
    "    discounted_r = discount_rewards(REWARDS)\n",
    "    \n",
    "    # training\n",
    "    CNN.fit(states, actions, sample_weight=discounted_r, epochs=1, verbose=0)\n",
    "    \n",
    "    # leeren des Episoden Buffers\n",
    "    STATES, ACTIONS, REWARDS = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainings des Agenten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-21.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial Mean Reward -> Mean Reward kann nich auf 0 gesetzt werden (Pong -21)\n",
    "INITIAL_MEAN_REWARD = 0.0\n",
    "env.reset()\n",
    "while True:\n",
    "    _, reward, done, _ = env.step(env.action_space.sample())\n",
    "    INITIAL_MEAN_REWARD += reward\n",
    "    if done:\n",
    "        break\n",
    "INITIAL_MEAN_REWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0524 09:39:49.889804 139881494673216 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:466: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Apply a constraint manually following the optimizer update step.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 \tReward: -21.0 \tMean: -21.0 \tBestMean: -21.0\n",
      "Episode: 2 \tReward: -20.0 \tMean: -20.5 \tBestMean: -21.0\n",
      "Episode: 3 \tReward: -20.0 \tMean: -20.33 \tBestMean: -20.5\n",
      "Episode: 4 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.333333333333332\n",
      "Episode: 5 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.333333333333332\n",
      "Episode: 6 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.333333333333332\n",
      "Episode: 7 \tReward: -21.0 \tMean: -20.57 \tBestMean: -20.333333333333332\n",
      "Episode: 8 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.333333333333332\n",
      "Episode: 9 \tReward: -21.0 \tMean: -20.56 \tBestMean: -20.333333333333332\n",
      "Episode: 10 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.333333333333332\n",
      "Episode: 11 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.333333333333332\n",
      "Episode: 12 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.333333333333332\n",
      "Episode: 13 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.333333333333332\n",
      "Episode: 14 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.333333333333332\n",
      "Episode: 15 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.333333333333332\n",
      "Episode: 16 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.333333333333332\n",
      "Episode: 17 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.333333333333332\n",
      "Episode: 18 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.333333333333332\n",
      "Episode: 19 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.333333333333332\n",
      "Episode: 20 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.333333333333332\n",
      "Episode: 21 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.333333333333332\n",
      "Episode: 22 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.333333333333332\n",
      "Episode: 23 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.333333333333332\n",
      "Episode: 24 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.333333333333332\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[7744,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node RMSprop/RMSprop/update_6/Square (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_4183]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a9dce5afa76d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTATES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACTIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mREWARDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mREWARD_LIST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_REWARD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-e53e46077d3f>\u001b[0m in \u001b[0;36mupdate_policy\u001b[0;34m(STATES, ACTIONS, REWARDS)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiscounted_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# leeren des Episoden Buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    318\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    119\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    120\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     return [_non_none_constant_value(out)\n\u001b[0;32m---> 84\u001b[0;31m             for out in distributed_function(input_fn)]\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    435\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1820\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[7744,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node RMSprop/RMSprop/update_6/Square (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_4183]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 10_000\n",
    "REWARD_LIST = []\n",
    "MEAN_LIST = []\n",
    "BEST_MEAN_REWARD = INITIAL_MEAN_REWARD\n",
    "STATES, ACTIONS, REWARDS = [], [], []\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    EPISODE_REWARD = 0.0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Reward einer Aktion zum gesamten Reward der Episode addieren\n",
    "        EPISODE_REWARD += reward\n",
    "        \n",
    "        # Transition zwischenspeichern\n",
    "        append_sample(state, action, reward)\n",
    "        \n",
    "        # State aktualisieren\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            update_policy(STATES, ACTIONS, REWARDS)\n",
    "            \n",
    "            REWARD_LIST.append(EPISODE_REWARD)\n",
    "            current_mean_reward = np.mean(REWARD_LIST[-min(len(REWARD_LIST), 10):])\n",
    "            MEAN_LIST.append(np.mean(REWARD_LIST))\n",
    "            \n",
    "            print(\"Episode:\", episode+1, \"\\tReward:\", EPISODE_REWARD, \"\\tMean:\", round(current_mean_reward, 2),\"\\tBestMean:\", BEST_MEAN_REWARD)\n",
    "\n",
    "            # Übernahme des höchsteb Mean Rewards\n",
    "            if current_mean_reward > BEST_MEAN_REWARD:\n",
    "                BEST_MEAN_REWARD = current_mean_reward\n",
    "\n",
    "                import os\n",
    "                try:\n",
    "                    os.makedirs(PATH)\n",
    "                except FileExistsError:\n",
    "                    # Pfad existiert bereits\n",
    "                    pass\n",
    "                CNN.save_weights(PATH +\"Best.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
