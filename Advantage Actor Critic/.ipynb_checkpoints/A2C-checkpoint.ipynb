{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOafjdahyQ4x"
   },
   "source": [
    "# Advantage Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6s7lP8w_qT-g"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isafSyFJqqbC"
   },
   "source": [
    "# **Auswahl des Spiels**\n",
    "\n",
    "[Hier](https://gym.openai.com/envs/#atari) ist eine vollständige Liste der verfügbaren Spiele zu finden. Um ein Environment zu erstellen muss der vollständige Name des Spiels als String übergeben werden.\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "```python\n",
    "game = \"MsPacman-v0\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oFwznyfqtY8"
   },
   "outputs": [],
   "source": [
    "# Hier kann das Spiel übergeben werden\n",
    "game = \"Breakout-v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZFpCCiUqxec"
   },
   "source": [
    "Der folgende Schritt ist notwendig, wenn das Notebook in Google Colab ausgeführt wird. Je nach installierter Version von OpenAIs Gym Bibliothek müssen die Atari-Spiele importiert werden, damit sie in Gym verfürbar sind. Die Spiele können [Hier](http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html) heruntergeladen werden und müssen anschließend in Google Drive hinterlegt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    env = gym.make(game)\n",
    "except:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !python -m atari_py.import_roms \"drive/MyDrive/Atari_Roms\"\n",
    "    env = gym.make(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing**\n",
    "Das Preprocessing der Daten ist enorm wichtig, da es einen großen Einfluss auf den Lernprozess des Agenten am Ende des Tages hat.\n",
    "Die Bibliothek Gym stellt hierfür Wrapper bereit, die die verschiedenen Spiele um Funktionalitäten erweitern können und das Lernen optimieren.\n",
    "\n",
    "Unter [Stable Baselines](https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py) sind schon vordefinierte Wrapper implementiert, die zum Training des Agenten nützlich sind. Die von \"Stable Baselines\" übernommenen Wrapper werden im folgenden kurz erläutert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K56-9gA8q2cM"
   },
   "source": [
    "### Fire Wrapper\n",
    "Der FireResetEnv Wrapper ist dafür zuständig, dass die \"FIRE\"-Aktion bei Spielreset ausgeführt wird. In manchen Spielen ist dies notwendig, damit das Spiel startet. Der Wrapper fragt zuvor die gültigen Aktionen ab und führt \"FIRE\" nur aus, wenn diese sich im Aktionsraum befindet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7xfJ9C1q47O"
   },
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env) \n",
    "        self.env.reset()\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        observation, _, _, _ = self.env.step(env.unwrapped.get_action_meanings().index('FIRE'))\n",
    "\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUX3_jS2MtNx"
   },
   "source": [
    "### MaxAndSkip Wrapper\n",
    "Aufeinanderfolgende Zustände in Atari-Spielen können schwer voneinander zu unterscheiden sein, da diese in kurzen Zeitabständen aufeinander folgen. Die Änderungen können so klein sein, dass diese für das Verhalten des Agenten nicht relevant erscheinen und deswegen übersprungen werden können. Der MaxAndSkipEnv Wrapper führt für N aufeinanderfolgende Aktionen die gleiche Aktion aus, da die Änderungen in diesem Zeitraum zu klein erscheinen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGNoWqC6MtNy"
   },
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Csj73MlMtN1"
   },
   "source": [
    "### ScaledFloatFrame Wrapper\n",
    "Künstliche neuronale Netze lernen effektiver, wenn sich die eingegbenen Werte in einem Intervall von 0 bis 1 oder -1 bis 1 befinden. Der ScaledFloatFrame Wrapper skalliert die Bilder auf ein vom Benutzer festgelegtes Intervall herunter. Das Intervall kann mit den Parametern \"low\" und \"high\" festgelegt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBYeRdvrMtN1"
   },
   "outputs": [],
   "source": [
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # careful! This undoes the memory optimization, use\n",
    "        # with smaller replay buffers only.\n",
    "        return np.array(observation).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vd03Hv1MtN2"
   },
   "source": [
    "### EpisodicLifeEnv Wrapper\n",
    "Manche Atari-Spiele lassen es zu, dass der Agent nach einer gescheiterten Aktion von einem Checkpoint aus startet. Das ermöglicht es ihm auch fortgeschrittene Zustände zu betreten und zu sampeln. Es kann aber auch vorkommen, dass er fehlerhafte Aktionen mit in das Lernen aufnimmt, da er so in fortgeschrittene Zustände kommt. Bei dem Spiel Breakout-v0 kommt es häufig vor, dass der Agent die ersten Bälle durchlässt, bevor er agiert. Der Agent positioniert sich dabei an einen Spielfeldrand, da der Spielball ab einen bestimmten Zeipunkt automatisch dort abgeworfen wird. Erst dann beginnt der Agent richtig zu spielen.\n",
    "Dieses Verhalten ist aber nicht immer erwünscht, deshalb kann der EpisodicLifeEnv Wrapper eingesetzt werden. Dieser Wrapper verhindert das Starten aus Checkpoints heraus und startet das Spiel neu sobald der Agent eine fehlerhafte Aktion begangen hat.\n",
    "Dies kann aber auch zum Nachteil haben, dass der Agent erst spät fortgeschrittene Spielzustände betritt und das Training zum angestrebten Leistungsziel verlängert wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3_tCT4fMtN2"
   },
   "outputs": [],
   "source": [
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done  = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
    "            # so it's important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_zaDmRJMtN4"
   },
   "source": [
    "### Clip Reward Wrapper\n",
    "Der ClipRewardEnv Wrapper beschneidet den Reward auf das Intervall von -1 bis 1. Dieses Verhalten kann erwünscht sein, wenn Rewards nicht zu stark von der Umgebung bewertet werden und der Agent vollständig aus eigenen Erfahrungen lernt die Zustände zu bewerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTsOOYyEMtN5"
   },
   "outputs": [],
   "source": [
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "        return np.sign(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukAIUaihrUBd"
   },
   "source": [
    "### Resize & Grayscale Wrapper\n",
    "Der WarpFrame Wrapper nimmt Einfluss auf die Beobachtung, die der Agent von der Umgebung erhält.\n",
    "Die Ausgangsbilder werden auf Bilder der Größe 84x84 Pixel herunterskaliert und anschleßend auf einen Farbkanal reduziert. Die Farben sind für das Lernen des Spiels nicht wichtig, sodass diese im Sinne einer besseren Rechenkapazität ausgelassen werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFE3KyHWrN-1"
   },
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "import cv2\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Warp frames to 84x84 as done in the Nature paper and later work.\n",
    "        :param env: (Gym Environment) the environment\n",
    "        \"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.height, self.width, 1),\n",
    "                                            dtype=env.observation_space.dtype)\n",
    "        \n",
    "    def observation(self, frame):\n",
    "        \"\"\"\n",
    "        returns the current observation from a frame\n",
    "        :param frame: ([int] or [float]) environment frame\n",
    "        :return: ([int] or [float]) the observation\n",
    "        \"\"\"\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Codezelle ist optional ausführbar und instanziirt eine Umgebung beispielhaft, um den Effekt des WarpFrame Wrapper darzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "kb1LXTXKs6qH",
    "outputId": "df0e09e1-a2f7-4c46-8cc7-9f6d35fcd6f0"
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "[OPTIONAL]\n",
    "\n",
    "Diese Zelle ist optional ausführbar und dient zur Visualisierung des Wrappers.\n",
    "Die Zelle hat keinen Einfluss auf den Agenten\n",
    "\"\"\"\n",
    "\n",
    "def WarpFrameEnv(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = WarpFrame(env)\n",
    "    return env\n",
    "\n",
    "normal_env = gym.make(game)\n",
    "wrapped_env = WarpFrameEnv(game)\n",
    "\n",
    "normal_env.reset()\n",
    "wrapped_env.reset()\n",
    "action = normal_env.action_space.sample()\n",
    "\n",
    "normal_state, _, _, _ = normal_env.step(action)\n",
    "wrapped_state, _, _, _ = wrapped_env.step(action)\n",
    "\n",
    "wrapped_state = wrapped_state[: , :, 0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.suptitle('Warp Frame', fontsize=20)\n",
    "axs[0].imshow(normal_state)\n",
    "axs[0].set_title(\"Normal\", fontsize=16)\n",
    "axs[1].imshow(wrapped_state, cmap=\"gray\")\n",
    "axs[1].set_title(\"Warp Frame\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjZQ6BOPq90B"
   },
   "source": [
    "### Frame Stack Wrapper\n",
    "Der FrameStack Wrapper stapelt N aufeinanderfolgende Zustände aufeinander, sodass diese dem Agenten zusammen als ein Zustand übergeben werden können.\n",
    "Das Stapeln der Zustände ist hilfreich, da der Agent so Bewegungen in der Umgebung wahrnehmen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYTs63ANrFyb"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.frames = deque(maxlen=4)\n",
    "        low = np.repeat(self.observation_space.low[np.newaxis, ...], repeats=4, axis=0)\n",
    "        high = np.repeat(self.observation_space.high[np.newaxis, ...], repeats=4, axis=0)\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=self.observation_space.dtype)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        frame_stack = np.asarray(self.frames, dtype=np.float32)\n",
    "        frame_stack = np.moveaxis(frame_stack, 0, -1).reshape(1, 84, 84, -1)\n",
    "        return frame_stack, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        for _ in range(4):\n",
    "            self.frames.append(obs)\n",
    "        frame_stack = np.asarray(self.frames, dtype=np.float32)\n",
    "        frame_stack = np.moveaxis(frame_stack, 0, -1).reshape(1, 84, 84, -1)\n",
    "        return frame_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Codezelle ist optional ausführbar und instanziirt eine Umgebung beispielhaft, um den Effekt des FrameStack Wrapper darzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "BFsMn2h-s04j",
    "outputId": "fca0dc94-9a39-4ee7-f49c-face6fa53783"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[OPTIONAL]\n",
    "\n",
    "Diese Zelle ist optional ausführbar und dient zur Visualisierung des Wrappers.\n",
    "Die Zelle hat keinen Einfluss auf den Agenten\n",
    "\"\"\"\n",
    "\n",
    "def FrameStackEnv(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = WarpFrame(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = FrameStack(env)\n",
    "    return env\n",
    "\n",
    "env = FrameStackEnv(game)\n",
    "env.reset()\n",
    "\n",
    "for _ in range(1, 5):\n",
    "  # Führe eine zufällige Aktion aus\n",
    "  state, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# Stack umformen, damit das Plotten der vier Bilder gelingt\n",
    "state = state.reshape(84, 84,4)\n",
    "\n",
    "# Frame Stack plotten\n",
    "fig, axs = plt.subplots(1,4, figsize=(15, 5))\n",
    "fig.suptitle('Frame Stack', fontsize=20)\n",
    "for i in range(state.shape[2]):\n",
    "    axs[i].imshow(state[:, :, i], cmap=\"gray\")\n",
    "    axs[i].set_title(\"Frame \"+str(i+1), fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1hORw-SrRKu"
   },
   "source": [
    "### Erstellen des Environments\n",
    "Hier werden die gewählten Wrapper an die Umgebung gebunden und diese anschließen instanziiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyVNp_0hrOPK",
    "outputId": "5e8359ff-a4cd-476d-ebbb-11a913d9b586"
   },
   "outputs": [],
   "source": [
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    #env = EpisodicLifeEnv(env)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    env = NoopResetEnv(env)\n",
    "    #env = ClipRewardEnv(env)\n",
    "    env = WarpFrame(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = FrameStack(env)\n",
    "    return env\n",
    "\n",
    "env = make_env(game)\n",
    "\n",
    "\"\"\" saving the properties for csv \"\"\"\n",
    "\n",
    "MODE = \"NoEpisodicLife_NoClipReward_lr_1e-3_5e-3\"\n",
    "PATH = \"WEIGHTS/\" + game + \"/\" + MODE + \"/\"\n",
    "print(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uAIJ4QYrja0"
   },
   "source": [
    "# Actor Network und Critic Network\n",
    "Hier wird die Kernkomponente des Agenten definiert, also das künstliche neuronale Netz. Zu Beginn speichern wir uns Parameter ab, die bei dem Erstellen des Netzes wichtig sind. \n",
    "\n",
    "- Der Parameter \"INPUT_SHAPE\" speichert die Dimension der Eingabe, also dem von der Umgebung erhaltenen Bild. Die Vier am Ende steht dafür, dass wir vier Bilder aufeinandergestapelt eingeben. Die Dimension der Eingabe hängt von den definierten Parametern im FrameStack Wrapper und WarpFrame Wrapper ab, da diese die Bildeingabe umformen.\n",
    "\n",
    "- Mit ACTOR_OUTPUT legen wir die Anzahl der Neuronen in der Ausgabeschicht des Actor-Netzwerkes fest. Das Actor-Netz braucht ein künstliches Neuron für jede gültige Aktion, die das Spiel zulässt. Mit env.action_space.n können wir die gültigen Aktionen der Umgebungsinstanz abfragen.\n",
    "\n",
    "- CRITIC_OUTPUT legt die Anzahl der künstlichen Neuronen in der Ausgabeschicht des Critic-Netzwerkes fest. Das Critic-Netzwerk braucht nur ein Ausgabeneuron, da es mit diesem eine Abschätzung über die Güte des aktuellen Zustandes trifft.\n",
    "\n",
    "- Zu jeweils beiden Netzwerken wird eine Lernrate definiert, die die Schrittgröße des Gradientenabstiegs während des Trainings der Netze bestimmt.\n",
    "\n",
    "\n",
    "Im nächsten Schritt werden die neuronalen Netze bausteinartig zusammengesetzt. Mit net_input definieren wir die Eingabedimension, die die Eingabeschichten annehmen sollen. Conv2D sind Faltungsschichten, die sich besonders gut dafür eignen Bilder zu verarbeiten.\n",
    "- Der Parameter filters gibt an, wie viele Filter in der Faltungsschicht zur Verarbeitung der Daten genutzt werden sollen.\n",
    "- kernel_size bestimmt die Größe der eingesetzten Filter.\n",
    "- strides beschreibt die Schrittweite, die ein Filter über die Eingabe verschoben wird.\n",
    "- padding legt fest, ob die ursprüngleiche Dimensionalität der Eingaben beibehalten werden soll.\n",
    "\n",
    "Nach jedem instanzierten Schichtaufruf wird die Eingabe der Schicht angegben. Beispielsweise werden unsere Schichten der Variable x zugewiesen und der nächsten Schicht übergeben. Da immer erst der linke Teil eines Gleicheheitszeichen aufgerufen wird, müssen wir nicht für jede unserer Schichten eine neue Variable definieren.\n",
    "\n",
    "Nach jeder Schicht wird mit einer Aktivierungsfunktion die Aktivierung der Neuronen in einer Schicht berechnet. Die gängigste Aktivierungsfuntion ist dabei die ReLU-Aktivierungsfunktion.\n",
    "\n",
    "Mit dem Aufruf von Flatten werden die berechneten Daten in ein Vektorformat gebracht, sodass sie von einer Dense-Schicht weiterverarbeitet werden können. Dense Schichten sind die \"einfachsten\" Schichten eines neuronalen Netzes und sind in der Literatur unter Feedfroward-Schicht zu finden. Der Parameter units gibt an, wie viele künstliche Neuronen sich in einer Dense-Schicht befinden sollen.\n",
    "\n",
    "Der Aufbau der Netze wird an der Ausgabeschicht aufgeteilt. Beide bekommen für den Parameter \"units\" jeweils den vorher definierten Parameter ACTOR_OUTPUT oder CRITIC_OUTPUT übergeben.\n",
    "Das Actor-Netzwerk nutzt die SoftMax-Verteilung als Aktivierungsfunktion an der Ausgabeschicht. Die SofMAx-Verteilung eignet sich zur Berechnung von Wahrscheinlichkeiten über eine gegebene Anzahl von Klassen. (Hier Aktionen)\n",
    "Das Critic-Netzwerk bekommt als Aktivierungsfunktion \"linear\" übergeben. Das ist mit dem Vergeben keiner Aktivierungsfunktion gleichzusetzen, da so der \"rohe\" berecnete Wert des Netzes ausgegben wird. Die Ausgabe beschreibt die Bewertung des eingegebenen Zustand.\n",
    "\n",
    "Zum Schluss instanziieren wir das neurnale Netz mit Model() und geben dabei die Eingabe- und die Ausgabeschicht an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGNNFrCgr0MM"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Network Parameter\n",
    "INPUT_SHAPE = (84, 84, 4) # (Höhe, Breite, Frames)\n",
    "ACTOR_OUTPUT = env.action_space.n # Anzahl der möglichen Aktionen\n",
    "CRITIC_OUTPUT = 1 # Bewertung der gewählten Aktion\n",
    "ACTOR_LEARNING_RATE = 25e-6\n",
    "CRITIC_LEARNING_RATE = 25e-6\n",
    "\n",
    "# neuronales Netz\n",
    "net_input = Input(shape=INPUT_SHAPE)\n",
    "x = Conv2D(filters=32, kernel_size=(8, 8), strides=(4, 4), padding=\"same\")(net_input)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(filters=64, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(units=512)(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "\n",
    "'''Aufspalten des Netzes in Actor und Critic'''\n",
    "\n",
    "\"\"\" Actor - wählt eine Aktion \"\"\" \n",
    "actor_x = Dense(units=ACTOR_OUTPUT, kernel_initializer='he_uniform')(x)\n",
    "actor_output = Activation(\"softmax\")(actor_x)\n",
    "\n",
    "ACTOR = Model(inputs=net_input, outputs=actor_output)\n",
    "ACTOR.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=ACTOR_LEARNING_RATE))\n",
    "\n",
    "\"\"\" Critic - bewertet gewählte Aktion \"\"\"\n",
    "critic_x = Dense(units=CRITIC_OUTPUT)(x)\n",
    "critic_output = Activation(\"linear\")(critic_x)\n",
    "\n",
    "CRITIC = Model(inputs=net_input, outputs=critic_output)\n",
    "CRITIC.compile(loss=\"mse\", optimizer=Adam(lr=CRITIC_LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s30U2V0CMtN_"
   },
   "source": [
    "## Aktion wählen\n",
    "Das Faltungsnetzwerk berechnet zu jedem Zustand eine Wahrscheinlichkeitsverteilung über die gültigen Aktionen. Die Wahl der auszuführenden Aktion wird von dieser Wahrscheinlichkeitsverteilung beeinflusst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEwfPKljMtN_"
   },
   "outputs": [],
   "source": [
    "def get_action(state):\n",
    "    \"\"\" Berechnen einer Wahrscheinlichkeitsverteilung durch das Actor-Netzwerk\"\"\"\n",
    "    policy = ACTOR.predict(state)[0]\n",
    "    \n",
    "    \"\"\" Wählen einer Aktion, die durch die berechnete Wahrscheinlichkeitsverteilung beeinflusst wird\"\"\"\n",
    "    action = np.random.choice(env.action_space.n, p=policy)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier werden die gesammelten Erfahrungen einer Episode in Listen gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLfgqEutNr10"
   },
   "outputs": [],
   "source": [
    "def remember (state, action, reward):\n",
    "    STATES.append(state)\n",
    "    \"\"\"Erstellen eines One Hot Labels für die Aktion\"\"\"\n",
    "    action_onehot = np.zeros([env.action_space.n])\n",
    "    action_onehot[action] = 1\n",
    "    ACTIONS.append(action_onehot)\n",
    "    REWARDS.append(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discount Rewards\n",
    "Für jeden durchlaufenen Zustand, in einer Episode, wird der diskontierte Reward berechnet. Das diskontieren der Zustände beeinflusst den Agenten bei der Aktionswahl. Je höher der discount-Faktor GAMMA definiert wird, desto stärker bezieht der Agent in der Zukunft liegende Rewards in seiner Aktionswahl mit ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRPapa6lR4-K"
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards):\n",
    "    gamma = 0.99\n",
    "    running_add = 0 # Reward bis zu einem Zustand aus der Liste\n",
    "    \n",
    "    \"\"\"\n",
    "        anlegen einer leeren Liste, in der später die diskontierten \n",
    "        Rewards gespeichert werden\n",
    "    \"\"\"\n",
    "    discounted_r = np.zeros_like(rewards)\n",
    "    \n",
    "    \"\"\" Berechnung des diskontierten Rewards für einen Zustand \"\"\"\n",
    "    for i in reversed(range(0, len(rewards))):\n",
    "        running_add = running_add * gamma + rewards[i]\n",
    "        discounted_r[i] = running_add\n",
    "\n",
    "    discounted_r -= np.mean(discounted_r)\n",
    "    discounted_r /= np.std(discounted_r)\n",
    "\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Episodes\n",
    "Zu Beginn werden die diskontierten Rewards einer Episode berechnet. Das Critic-Netzwerk berechnet eine Bewertung für die in der Episode durchlaufenden Zustände, die in der Vairable \"values\" abgespeichert werden.\n",
    "\n",
    "Die Advantage-Werte ergeben sich dann aus der Differenz der diskontierten Rewards (zu erwartender Gesamtreward von einem Zustand aus) und der vom Critic-Netzwerk berechneten Bewertung.\n",
    "\n",
    "Danach werden beide Netze trainiert. Das Actor-Netzwerk bekommt die gewählten Aktionen als Zielwerte übergeben. Die diskontierten Rewards werden dem Actor-Netzwerk zusätzlich als Wichtung für jedes Datenpaar mit übergeben, um die Aktionswahl\n",
    "in Richtung des höchsten diskontierten Rewards zu drängen.\n",
    "Das Crtic-Netzwerk bekommt als Zielwerte die diskontierten Rewards übergeben. Diese stellen den zu erwartenden Reward für einen Zustand dar und sind somit ein gutes Maß zur Zustandsbewertung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3SGt8rvRTPT"
   },
   "outputs": [],
   "source": [
    "def replay_episode(STATES, ACTIONS, REWARDS):\n",
    "    \"\"\" Daten in ein für das neuronale Netz kompatibles Format bringen\"\"\"\n",
    "    states = np.vstack(STATES)\n",
    "    actions = np.vstack(ACTIONS)\n",
    "\n",
    "    \"\"\" Daten in ein für das neuronale Netz kompatibles Format bringen\"\"\"\n",
    "    discounted_r = discount_rewards(REWARDS)\n",
    "\n",
    "    \"\"\" Bewertung der Zustände durch das Critic-Netzwerk\"\"\"\n",
    "    values = CRITIC.predict(states)[:, 0]\n",
    "\n",
    "    \"\"\" berechnen der Advantage-Werte aus den diskontierten Rewards und den Zustandsbewertungen \"\"\"\n",
    "    advantages = discounted_r - values\n",
    "\n",
    "    \"\"\" Trainieren der Netzwerke ACTOR und CRITIC\"\"\"\n",
    "    ACTOR.fit(states, actions, sample_weight=advantages, epochs=1, verbose=0)\n",
    "    CRITIC.fit(states, discounted_r, epochs=1, verbose=0)\n",
    "\n",
    "    \"\"\" leeren des Episoden Buffers\"\"\"\n",
    "    STATES, ACTIONS, REWARDS = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwLX4bvUMtOA"
   },
   "source": [
    "# Training /Spielen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fV_l4s5aMtOA",
    "outputId": "7dfdb8a8-f590-44df-a460-124c11c396d4"
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Speichern eines initialen Mean Rewards. Während des Trainings wird der Mean Reward aus den \n",
    "letzten zehn gespielten Episoden berechnet. Ist der berechnete Mean Reward besser, so werden die\n",
    "Netzparameter gespeichert und der Mean Reward mit dem aktuellen überschrieben.\n",
    "\n",
    "Der initiale Mean Reward kann nicht auf 0 gesetzt werden, da der Score des Spiels Pong bei -21 startet.\n",
    "Durch Spielen einer zufälligenen Episode samplen wir einen Beispielhaften aber schlechten Reward zum Beginnen.\n",
    "\"\"\"\n",
    "INITIAL_MEAN_REWARD = 0.0\n",
    "env.reset()\n",
    "while True:\n",
    "    _, reward, done, _ = env.step(env.action_space.sample())\n",
    "    INITIAL_MEAN_REWARD += reward\n",
    "    if done:\n",
    "        break\n",
    "INITIAL_MEAN_REWARD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu Beginn legen wir die Episodenanzahl fest, die der Agent zum Training durchlaufen soll.\n",
    "Die verschiedenen Listen werden angelegt, um Parameter abzuspeichern, an denen das Training abschließend bewertet werden kann.\n",
    "\n",
    "Wir starten den Trainingsprozess mit einer for-Schleife für EPISODES viele Iterationen. Zu Beginn jeder Iteration wird das Spiel neu gestartet, da eine Episode pro Iteration gespielt wird. Die \"done\"-Flag wird auf False gesetzt, da wir uns nicht länger in einem Endzustand befinden.\n",
    "\n",
    "Die while-Schleife leitet dann eine Episode ein und wird so lange durchlaufen, bis die \"done\"-Flag das Ende einer Episode signalisiert.\n",
    "\n",
    "Zu Beginn jeder Episode wählen wir eine Aktion auf Grundlage des Initialzustandes (state = env.reset()). Während der Episode werden die Aktionen anschließend anhand des aktuellen Episodenzustandes gewählt (next_state, ... = env.step(action)).\n",
    "Durch das Interagieren mit der Umgebung erhält der Agent eine neue Beobachtung (next_state), eine Belohnung (reward), eine Flag (done) und ein info-Dictionary (_) von der Umgebung. Das info-Dictionary enhält weitere Informationen, wie beispielsweise die Leben des Agenten, falls vorhanden. Das Dictionary wird nicht für das Training des Agenten benötigt.\n",
    "\n",
    "Die erhaltenen Parameter werden mit remember() in Listen abgespeichert und werden nach jeder Episode für das Training verwendet.\n",
    "\n",
    "Das Training des neuronalen Netzes wird durch die Methode \"replay()\" eingeleitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zXHmDN4bMtOA",
    "outputId": "78075f13-7208-4760-d7ca-46b75c91af46",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPISODES = 15_000\n",
    "REWARD_LIST = []\n",
    "MEAN_LIST = []\n",
    "BEST_MEAN_REWARD = INITIAL_MEAN_REWARD\n",
    "\n",
    "# for-Schleife des Trainingsprozesses\n",
    "for episode in range(EPISODES):\n",
    "    EPISODE_REWARD = 0.0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # Episoden Buffer\n",
    "    STATES, ACTIONS, REWARDS = [], [], []\n",
    "    \n",
    "    # while-Schleife einer Episode\n",
    "    while not done:\n",
    "        action = get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \"\"\" speichern einer Transition im Episodenspeicher \"\"\"\n",
    "        remember(state, action, reward)\n",
    "\n",
    "        \"\"\" aktualisieren des state Variable auf den neusten Umgebungszustand \"\"\"\n",
    "        state = next_state\n",
    "        \n",
    "        \"\"\" Reward einer Aktion zum gesamten Reward der Episode addieren \"\"\"\n",
    "        EPISODE_REWARD += reward\n",
    "\n",
    "\n",
    "        if done:\n",
    "            REWARD_LIST.append(EPISODE_REWARD)\n",
    "            current_mean_reward = np.mean(REWARD_LIST[-min(len(REWARD_LIST), 10):])\n",
    "            MEAN_LIST.append(np.mean(REWARD_LIST))\n",
    "            \n",
    "            \"\"\" Ausgabe des aktuellen Trainingsfortschrittes \"\"\"\n",
    "            print(\"Episode:\", episode+1, \"\\tReward:\", EPISODE_REWARD, \"\\tMean:\", round(current_mean_reward, 2),\"\\tBestMean:\", BEST_MEAN_REWARD)\n",
    "\n",
    "            \"\"\" Übernahme des höchsten Mean Rewards \"\"\"\n",
    "            if current_mean_reward > BEST_MEAN_REWARD:\n",
    "                BEST_MEAN_REWARD = current_mean_reward\n",
    "        \n",
    "                \"\"\" Trainierte Gewichte speichern \"\"\"\n",
    "                import os\n",
    "                try:\n",
    "                    os.makedirs(PATH)\n",
    "                except FileExistsError:\n",
    "                    # Pfad existiert bereits\n",
    "                    pass\n",
    "                ACTOR.save_weights(PATH + \"Best_ACTOR.h5\")\n",
    "                CRITIC.save_weights(PATH + \"Best_CRITIC.h5\")\n",
    "\n",
    "            replay_episode(STATES, ACTIONS, REWARDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-k4snaUMtOB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "date = datetime.now().date()\n",
    "\n",
    "df = pd.DataFrame(list(zip(REWARD_LIST, MEAN_LIST)), \n",
    "               columns =['Rewards', 'Mean Reward']) \n",
    "df.to_csv(PATH + game + \"_\" + str(date) + \"_\"+ MODE + \".csv\", mode=\"w\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1FmesqdMtOB"
   },
   "outputs": [],
   "source": [
    "ACTOR.save_weights(PATH + \"End.h5\")\n",
    "CRITIC.save_weights(PATH + \"End.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKSlrmpRMtOC"
   },
   "source": [
    "# Auswertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzDVaCvDMtOC"
   },
   "outputs": [],
   "source": [
    "\"\"\" Erstellen einer Grafik über den Trainingsverlauf \"\"\"\n",
    "plt.figure(figsize=(25, 12))\n",
    "plt.plot(REWARD_LIST, label=\"erhaltene Rewards\")\n",
    "plt.plot(MEAN_LIST, label=\"durchschnittler Reward\")\n",
    "plt.title(\"Rewards während des Trainings\", fontsize=25)\n",
    "plt.xlabel(\"Episoden\", fontsize=20)\n",
    "plt.ylabel(\"Rewards\", fontsize=20)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqvMdh7kMtOC"
   },
   "source": [
    "# Spielen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gewichte laden\n",
    "#WEIGHTS_PATH = \"WEIGHTS/...h5\"\n",
    "#ACTOR.load_weights(filepath=WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s23zjIfZMtOD"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "\n",
    "for i in range(1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        img = plt.imshow(env.render(mode='rgb_array'))\n",
    "        img.set_data(env.render(mode='rgb_array'))\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        action = np.argmax(ACTOR.predict(state))\n",
    "        state, reward, done, info = env.step(action)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Kopie von A2C.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
