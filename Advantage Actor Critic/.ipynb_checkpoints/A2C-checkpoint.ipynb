{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOafjdahyQ4x"
   },
   "source": [
    "# TODO:\n",
    "* Wrapper for reward clipping to [-1, 1] (same reason like image normalization)\n",
    "* checking hyperparameters from paper\n",
    "* checking used parameters\n",
    "* * Adding results to a Dataframe and save to hard drive (to compare with other models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6s7lP8w_qT-g"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 13 05:11:41 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 00000000:04:00.0 Off |                    0 |\r\n",
      "| N/A   51C    P0    71W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isafSyFJqqbC"
   },
   "source": [
    "# Auswahl des Spiels\n",
    "\n",
    "[Hier](https://gym.openai.com/envs/#atari) ist eine vollständige Liste der verfügbaren Spiele zu finden. Um ein Environment zu erstellen muss der vollständige Name des Spiels als String übergeben werden.\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "```python\n",
    "game = \"MsPacman-v0\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9oFwznyfqtY8"
   },
   "outputs": [],
   "source": [
    "# Hier kann das Spiel übergeben werden\n",
    "game = \"Pong-v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZFpCCiUqxec"
   },
   "source": [
    "# **Preprocessing**\n",
    "[Stable Baselines](https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K56-9gA8q2cM"
   },
   "source": [
    "## Fire Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "H7xfJ9C1q47O"
   },
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env) \n",
    "        self.env.reset()\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        observation, _, _, _ = self.env.step(env.unwrapped.get_action_meanings().index('FIRE'))\n",
    "\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max And Skip Env Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noop Reset Env Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Float Frame Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # careful! This undoes the memory optimization, use\n",
    "        # with smaller replay buffers only.\n",
    "        return np.array(observation).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic Life Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done  = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
    "            # so it's important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip Reward Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "        return np.sign(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukAIUaihrUBd"
   },
   "source": [
    "## Resize & Grayscale Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IFE3KyHWrN-1"
   },
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "import cv2\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Warp frames to 84x84 as done in the Nature paper and later work.\n",
    "        :param env: (Gym Environment) the environment\n",
    "        \"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.height, self.width, 1),\n",
    "                                            dtype=env.observation_space.dtype)\n",
    "        \n",
    "    def observation(self, frame):\n",
    "        \"\"\"\n",
    "        returns the current observation from a frame\n",
    "        :param frame: ([int] or [float]) environment frame\n",
    "        :return: ([int] or [float]) the observation\n",
    "        \"\"\"\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "kb1LXTXKs6qH",
    "outputId": "77e5d9bf-5428-4c85-e81f-9bedb73fe478"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" \n",
    "[OPTIONAL]\n",
    "\n",
    "Diese Zelle ist optional ausführbar und dient zur Visualisierung des Wrappers.\n",
    "Die Zelle hat keinen Einfluss auf den Agenten\n",
    "\"\"\"\n",
    "\n",
    "def WarpFrameEnv(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = WarpFrame(env)\n",
    "    return env\n",
    "\n",
    "normal_env = gym.make(game)\n",
    "wrapped_env = WarpFrameEnv(game)\n",
    "\n",
    "normal_env.reset()\n",
    "wrapped_env.reset()\n",
    "action = normal_env.action_space.sample()\n",
    "\n",
    "normal_state, _, _, _ = normal_env.step(action)\n",
    "wrapped_state, _, _, _ = wrapped_env.step(action)\n",
    "\n",
    "wrapped_state = wrapped_state[: , :, 0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.suptitle('Warp Frame', fontsize=20)\n",
    "axs[0].imshow(normal_state)\n",
    "axs[0].set_title(\"Normal\", fontsize=16)\n",
    "axs[1].imshow(wrapped_state, cmap=\"gray\")\n",
    "axs[1].set_title(\"Warp Frame\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjZQ6BOPq90B"
   },
   "source": [
    "## Frame Stack Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qYTs63ANrFyb"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.frames = deque(maxlen=4)\n",
    "        low = np.repeat(self.observation_space.low[np.newaxis, ...], repeats=4, axis=0)\n",
    "        high = np.repeat(self.observation_space.high[np.newaxis, ...], repeats=4, axis=0)\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=self.observation_space.dtype)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        frame_stack = np.asarray(self.frames, dtype=np.float32)\n",
    "        frame_stack = np.moveaxis(frame_stack, 0, -1).reshape(1, 84, 84, -1)\n",
    "        return frame_stack, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        for _ in range(4):\n",
    "            self.frames.append(obs)\n",
    "        frame_stack = np.asarray(self.frames, dtype=np.float32)\n",
    "        frame_stack = np.moveaxis(frame_stack, 0, -1).reshape(1, 84, 84, -1)\n",
    "        return frame_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "BFsMn2h-s04j",
    "outputId": "1b951e75-af6d-41d4-ab34-911969d424b8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "[OPTIONAL]\n",
    "\n",
    "Diese Zelle ist optional ausführbar und dient zur Visualisierung des Wrappers.\n",
    "Die Zelle hat keinen Einfluss auf den Agenten\n",
    "\"\"\"\n",
    "\n",
    "def FrameStackEnv(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = WarpFrame(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = FrameStack(env)\n",
    "    return env\n",
    "\n",
    "env = FrameStackEnv(game)\n",
    "env.reset()\n",
    "\n",
    "for _ in range(1, 5):\n",
    "  # Führe eine zufällige Aktion aus\n",
    "  state, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# Stack umformen, damit das Plotten der vier Bilder gelingt\n",
    "state = state.reshape(84, 84,4)\n",
    "\n",
    "# Frame Stack plotten\n",
    "fig, axs = plt.subplots(1,4, figsize=(15, 5))\n",
    "fig.suptitle('Frame Stack', fontsize=20)\n",
    "for i in range(state.shape[2]):\n",
    "    axs[i].imshow(state[:, :, i], cmap=\"gray\")\n",
    "    axs[i].set_title(\"Frame \"+str(i+1), fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1hORw-SrRKu"
   },
   "source": [
    "## Erstellen des Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uyVNp_0hrOPK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTS/Pong-v0/NoEpisodicLife_NoClipReward_opti_wo_lr/\n"
     ]
    }
   ],
   "source": [
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    #env = EpisodicLifeEnv(env)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    env = NoopResetEnv(env)\n",
    "    #env = ClipRewardEnv(env)\n",
    "    env = WarpFrame(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = FrameStack(env)\n",
    "    return env\n",
    "\n",
    "env = make_env(game)\n",
    "\n",
    "\"\"\" saving the properties for csv \"\"\"\n",
    "\n",
    "MODE = \"NoEpisodicLife_NoClipReward_lr_1e-3_5e-3\"\n",
    "PATH = \"WEIGHTS/\" + game + \"/\" + MODE + \"/\"\n",
    "print(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uAIJ4QYrja0"
   },
   "source": [
    "# Actor Network und Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aGNNFrCgr0MM"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Network Parameter\n",
    "INPUT_SHAPE = (84, 84, 4) # (Höhe, Breite, Frames)\n",
    "ACTOR_OUTPUT = env.action_space.n # Anzahl der möglichen Aktionen\n",
    "CRITIC_OUTPUT = 1 # Bewertung der gewählten Aktion\n",
    "ACTOR_LEARNING_RATE = 1e-3\n",
    "CRITIC_LEARNING_RATE = 5e-3\n",
    "\n",
    "# neuronales Netz\n",
    "net_input = Input(shape=INPUT_SHAPE)\n",
    "x = Conv2D(filters=32, kernel_size=(8, 8), strides=(4, 4), padding=\"same\")(net_input)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(filters=64, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256)(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "\n",
    "'''Aufspalten des Netzes in Actor und Critic'''\n",
    "\n",
    "# Actor - wählt eine Aktion\n",
    "actor_x = Dense(ACTOR_OUTPUT)(x)\n",
    "actor_output = Activation(\"softmax\")(actor_x)\n",
    "\n",
    "ACTOR = Model(inputs=net_input, outputs=actor_output)\n",
    "ACTOR.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=ACTOR_LEARNING_RATE))\n",
    "\n",
    "# Critic - bewertet gewählte Aktion\n",
    "critic_x = Dense(CRITIC_OUTPUT)(x)\n",
    "critic_output = Activation(\"linear\")(critic_x)\n",
    "\n",
    "CRITIC = Model(inputs=net_input, outputs=critic_output)\n",
    "CRITIC.compile(loss=\"mse\", optimizer=Adam(lr=CRITIC_LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aktion wählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state):\n",
    "    policy = ACTOR.predict(state)[0]\n",
    "    action = np.random.choice(env.action_space.n, p=policy) # Aktionen, welche Wahrscheinlichkeit zu Aktion\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "\n",
    "def update_policy(state, action, reward, next_state, done):\n",
    "    values = np.zeros(shape=(1, CRITIC_OUTPUT))\n",
    "    advantages = np.zeros(shape=(1, ACTOR_OUTPUT))\n",
    "    \n",
    "    # State bewerten\n",
    "    value = CRITIC.predict(state)[0]\n",
    "    next_value = CRITIC.predict(next_state)[0]\n",
    "    \n",
    "    if done:\n",
    "        advantages[0][action] = reward - value\n",
    "        values[0][0] = reward\n",
    "    else:\n",
    "        advantages[0][action] = (reward + GAMMA * next_value) - value\n",
    "        values[0][0] = reward + GAMMA * next_value\n",
    "        \n",
    "    # Trainieren der Netze\n",
    "    ACTOR.fit(state, advantages, verbose=0)\n",
    "    CRITIC.fit(state, values, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training /Spielen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial Mean Reward -> Mean Reward kann nich auf 0 gesetzt werden (Pong -21)\n",
    "INITIAL_MEAN_REWARD = 0.0\n",
    "env.reset()\n",
    "while True:\n",
    "    _, reward, done, _ = env.step(env.action_space.sample())\n",
    "    INITIAL_MEAN_REWARD += reward\n",
    "    if done:\n",
    "        break\n",
    "INITIAL_MEAN_REWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0413 05:11:51.514765 139768438773568 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:466: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Apply a constraint manually following the optimizer update step.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 2 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 3 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 4 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 5 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 6 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 7 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 8 \tReward: -20.0 \tMean: -20.88 \tBestMean: -20.0\n",
      "Episode: 9 \tReward: -21.0 \tMean: -20.89 \tBestMean: -20.0\n",
      "Episode: 10 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 11 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 12 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 13 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 14 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 15 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 16 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 17 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 18 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 19 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 20 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 21 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 22 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 23 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 24 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 25 \tReward: -20.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 26 \tReward: -20.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 27 \tReward: -21.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 28 \tReward: -20.0 \tMean: -20.2 \tBestMean: -20.0\n",
      "Episode: 29 \tReward: -21.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 30 \tReward: -20.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 31 \tReward: -21.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 32 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 33 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 34 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 35 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 36 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 37 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 38 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 39 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 40 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 41 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 42 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 43 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 44 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 45 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 46 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 47 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 48 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 49 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 50 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 51 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 52 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 53 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 54 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 55 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 56 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 57 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 58 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 59 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 60 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 61 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 62 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 63 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 64 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 65 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 66 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 67 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 68 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 69 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 70 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 71 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 72 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 73 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 74 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 75 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 76 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 77 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 78 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 79 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 80 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 81 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 82 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 83 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 84 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 85 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 86 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 87 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 88 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 89 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 90 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 91 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 92 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 93 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 94 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 95 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 96 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 97 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 98 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 99 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 100 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 101 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 102 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 103 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 104 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 105 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 106 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 107 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 108 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 109 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 110 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 111 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 112 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 113 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 114 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 115 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 116 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 117 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 118 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 119 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 120 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 121 \tReward: -20.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 122 \tReward: -20.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 123 \tReward: -21.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 124 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 125 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 126 \tReward: -20.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 127 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 128 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 129 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 130 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 131 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 132 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 133 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 134 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 135 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 136 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 137 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 138 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 139 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 140 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 141 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 142 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 143 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 144 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 145 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 146 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 147 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 148 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 149 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 150 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 151 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 152 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 153 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 154 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 155 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 156 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 157 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 158 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 159 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 160 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 161 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 162 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 163 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 164 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 165 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 166 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 167 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 168 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 169 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 170 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 171 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 172 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 173 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 174 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 175 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 176 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 177 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 178 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 179 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 180 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 181 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 182 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 183 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 184 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 185 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 186 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 187 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 188 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 189 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 190 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 191 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 192 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 193 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 194 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 195 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 196 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 197 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 198 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 199 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 200 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 201 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 202 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 203 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 204 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 205 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 206 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 207 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 208 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 209 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 210 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 211 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 212 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 213 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 214 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 215 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 216 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 217 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 218 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 219 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 220 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 221 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 222 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 223 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 224 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 225 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 226 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 227 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 228 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 229 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 230 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 231 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 232 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 233 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 234 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 235 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 236 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 237 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 238 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 239 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 240 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 241 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 242 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 243 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 244 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 245 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 246 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 247 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 248 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 249 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 250 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 251 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 252 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 253 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 254 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 255 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 256 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 257 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 258 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 259 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 260 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 261 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 262 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 263 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 264 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 265 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 266 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 267 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 268 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 269 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 270 \tReward: -20.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 271 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 272 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 273 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 274 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 275 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 276 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 277 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 278 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 279 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 280 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 281 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 282 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 283 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 284 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 285 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 286 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 287 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 288 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 289 \tReward: -20.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 290 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 291 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 292 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 293 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 294 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 295 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 296 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 297 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 298 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 299 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 300 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 301 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 302 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 303 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 304 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 305 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 306 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 307 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 308 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 309 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 310 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 311 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 312 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 313 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 314 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 315 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 316 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 317 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 318 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 319 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 320 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 321 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 322 \tReward: -20.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 323 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 324 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 325 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 326 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 327 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 328 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 329 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 330 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 331 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 332 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 333 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 334 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 335 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 336 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 337 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 338 \tReward: -20.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 339 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 340 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 341 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 342 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 343 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 344 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 345 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 346 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 347 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 348 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 349 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 350 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 351 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 352 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 353 \tReward: -20.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 354 \tReward: -21.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 355 \tReward: -21.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 356 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 357 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 358 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 359 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 360 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 361 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 362 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 363 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 364 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 365 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 366 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 367 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 368 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 369 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 370 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 371 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 372 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 373 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 374 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 375 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 376 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 377 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 378 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 379 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 380 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 381 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 382 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 383 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 384 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 385 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 386 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 387 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 388 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 389 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 390 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 391 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 392 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 393 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 394 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 395 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 396 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 397 \tReward: -21.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 398 \tReward: -21.0 \tMean: -21.0 \tBestMean: -20.0\n",
      "Episode: 399 \tReward: -20.0 \tMean: -20.9 \tBestMean: -20.0\n",
      "Episode: 400 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 401 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 402 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 403 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 404 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 405 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 406 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 407 \tReward: -19.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 408 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 409 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 410 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 411 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 412 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 413 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 414 \tReward: -19.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 415 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 416 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 417 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 418 \tReward: -19.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 419 \tReward: -20.0 \tMean: -20.2 \tBestMean: -20.0\n",
      "Episode: 420 \tReward: -21.0 \tMean: -20.2 \tBestMean: -20.0\n",
      "Episode: 421 \tReward: -21.0 \tMean: -20.2 \tBestMean: -20.0\n",
      "Episode: 422 \tReward: -21.0 \tMean: -20.2 \tBestMean: -20.0\n",
      "Episode: 423 \tReward: -20.0 \tMean: -20.2 \tBestMean: -20.0\n",
      "Episode: 424 \tReward: -20.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 425 \tReward: -20.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 426 \tReward: -21.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 427 \tReward: -19.0 \tMean: -20.2 \tBestMean: -20.0\n",
      "Episode: 428 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 429 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 430 \tReward: -20.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 431 \tReward: -21.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 432 \tReward: -20.0 \tMean: -20.2 \tBestMean: -20.0\n",
      "Episode: 433 \tReward: -19.0 \tMean: -20.1 \tBestMean: -20.0\n",
      "Episode: 434 \tReward: -20.0 \tMean: -20.1 \tBestMean: -20.0\n",
      "Episode: 435 \tReward: -20.0 \tMean: -20.1 \tBestMean: -20.0\n",
      "Episode: 436 \tReward: -21.0 \tMean: -20.1 \tBestMean: -20.0\n",
      "Episode: 437 \tReward: -21.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 438 \tReward: -20.0 \tMean: -20.2 \tBestMean: -20.0\n",
      "Episode: 439 \tReward: -21.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 440 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 441 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 442 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 443 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 444 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 445 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 446 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 447 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 448 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 449 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 450 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 451 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 452 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 453 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 454 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 455 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 456 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 457 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 458 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 459 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 460 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 461 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 462 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 463 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 464 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 465 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 466 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 467 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 468 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 469 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 470 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 471 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 472 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 473 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 474 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 475 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 476 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 477 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 478 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 479 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 480 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 481 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 482 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 483 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 484 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 485 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 486 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 487 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 488 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 489 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 490 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 491 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 492 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 493 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 494 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 495 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 496 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 497 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 498 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 499 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 500 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 501 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 502 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 503 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 504 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 505 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 506 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 507 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 508 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 509 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 510 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 511 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 512 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 513 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 514 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 515 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 516 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 517 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 518 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 519 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 520 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 521 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 522 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 523 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 524 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 525 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 526 \tReward: -21.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 527 \tReward: -20.0 \tMean: -20.8 \tBestMean: -20.0\n",
      "Episode: 528 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 529 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 530 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 531 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 532 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 533 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 534 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 535 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 536 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 537 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 538 \tReward: -20.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 539 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 540 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 541 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 542 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 543 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 544 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 545 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 546 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 547 \tReward: -20.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 548 \tReward: -20.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 549 \tReward: -21.0 \tMean: -20.3 \tBestMean: -20.0\n",
      "Episode: 550 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 551 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 552 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 553 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 554 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 555 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 556 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 557 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 558 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 559 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 560 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 561 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 562 \tReward: -21.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 563 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 564 \tReward: -20.0 \tMean: -20.4 \tBestMean: -20.0\n",
      "Episode: 565 \tReward: -21.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 566 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 567 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 568 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 569 \tReward: -21.0 \tMean: -20.7 \tBestMean: -20.0\n",
      "Episode: 570 \tReward: -20.0 \tMean: -20.6 \tBestMean: -20.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 571 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 572 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 573 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 574 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 575 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n",
      "Episode: 576 \tReward: -20.0 \tMean: -20.5 \tBestMean: -20.0\n",
      "Episode: 577 \tReward: -21.0 \tMean: -20.6 \tBestMean: -20.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b65eb8a73668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-63669ddcaebd>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mACTOR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Aktionen, welche Wahrscheinlichkeit zu Aktion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "EPISODES = 15000\n",
    "REWARD_LIST = []\n",
    "MEAN_LIST = []\n",
    "BEST_MEAN_REWARD = INITIAL_MEAN_REWARD\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    EPISODE_REWARD = 0.0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Transition im MEMORY BUFFER speichern\n",
    "        update_policy(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Reward einer Aktion zum gesamten Reward der Episode addieren\n",
    "        EPISODE_REWARD += reward\n",
    "        \n",
    "        # State aktualisieren\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            REWARD_LIST.append(EPISODE_REWARD)\n",
    "            current_mean_reward = np.mean(REWARD_LIST[-min(len(REWARD_LIST), 10):])\n",
    "            MEAN_LIST.append(np.mean(REWARD_LIST))\n",
    "            \n",
    "            print(\"Episode:\", episode+1, \"\\tReward:\", EPISODE_REWARD, \"\\tMean:\", round(current_mean_reward, 2),\"\\tBestMean:\", BEST_MEAN_REWARD)\n",
    "\n",
    "            # Übernahme des höchsten Mean Rewards\n",
    "            if current_mean_reward > BEST_MEAN_REWARD:\n",
    "                BEST_MEAN_REWARD = current_mean_reward\n",
    "        \n",
    "                # trainierte Gewichte speichern\n",
    "                import os\n",
    "                try:\n",
    "                    os.makedirs(PATH)\n",
    "                except FileExistsError:\n",
    "                    # Pfad existiert bereits\n",
    "                    pass\n",
    "                ACTOR.save_weights(PATH + \"Best_ACTOR.h5\")\n",
    "                CRITIC.save_weights(PATH + \"Best_CRITIC.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "date = datetime.now().date()\n",
    "\n",
    "df = pd.DataFrame(list(zip(REWARD_LIST, MEAN_LIST)), \n",
    "               columns =['Rewards', 'Mean Reward']) \n",
    "df.to_csv(PATH + game + \"_\" + str(date) + \"_\"+ MODE + \".csv\", mode=\"w\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTOR.save_weights(PATH + \"End.h5\")\n",
    "CRITIC.save_weights(PATH + \"End.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auswertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 12))\n",
    "plt.plot(REWARD_LIST, label=\"erhaltene Rewards\")\n",
    "plt.plot(MEAN_LIST, label=\"durchschnittler Reward\")\n",
    "plt.title(\"Rewards während des Trainings\", fontsize=25)\n",
    "plt.xlabel(\"Episoden\", fontsize=20)\n",
    "plt.ylabel(\"Rewards\", fontsize=20)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spielen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAAD8CAYAAAA/rZtiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAObklEQVR4nO3df4wc9XnH8fcnJgYJ0uAf1EJgikFOJINah1iU/ADRUhJwqhj6B7VViJOiHkggBSVVZUBqUKVIaRpAitoSgbBiAjXQEgJSnBTXioKiAMEQB8wPgw1G+GTs5IiAhijE5ukf871kfNxy62d2vbPbz0s67ex3Znaeke/j+XG7zyoiMLOD855BF2A2jBwcswQHxyzBwTFLcHDMEhwcs4S+BUfSeZK2SdouaU2/tmM2COrH33EkzQKeA84FdgGPAqsi4umeb8xsAPp1xDkd2B4RL0TEW8CdwIo+bcvskDusT697HPBy7fku4E87LSzpXQ97C/9gVo/KMuvey6/v/0VEHDPdvH4FZ0aSxoAxgDlHvIcvnf3+QZUyrXM/+pGDXmfjjx/qQyXDb/MXPnXQ6yy74bt9qOTgXPX9X77UaV6/TtXGgYW158eXsd+JiJsjYllELDtqtvpUhll/9Cs4jwKLJS2SNBtYCdzfp22ZHXJ9OVWLiH2SrgT+G5gFrI2Ip/qxLbNB6Ns1TkRsADb06/UPtemuXzLXQTb99UvmOmiQ/M4BswQHxyzBwTFLGNjfcYaNr2eszkccswQHxyzBwTFL8DVOl/x3HKvzEccswcExS3BwzBIcHLME3xzokm8E9M6wvaFzOj7imCU4OGYJDo5Zgq9xOnDjjd5pQ+ONXksfcSQtlPQDSU9LekrS58v4dZLGJW0pP8t7V65ZOzQ54uwDvhgRj0t6H/CYpI1l3o0R8bXm5Zm1Uzo4EbEb2F2m35D0DFUjwoM2d9GpXHz7pmwpZn1x1fz5Hef15OaApBOBDwGPlKErJT0haa2kOb3YhlmbNA6OpKOAe4CrIuJ14CbgZGAp1RHp+g7rjUnaLGnzxMRE0zLMDqlGwZH0XqrQ3BER3waIiD0RsT8i3gZuoWrA/g71Tp7z5s1rUobZIdfkrpqAW4FnIuKG2vixtcUuBLbmyzNrpyZ31T4GXAI8KWlLGbsGWCVpKRDATuCyRhWatVCTu2o/Aqbrlj4y3TvNOvFbbswSHByzBAfHLKEVb/J89cWt3H7x4kGXYdY1H3HMEhwcswQHxyzBwTFLcHDMEhwcswQHxyzBwTFLcHDMEhwcswQHxyzBwTFLcHDMEhq/O1rSTuANYD+wLyKWSZoL3AWcSPXx6Ysi4pdNt2XWFr064vxZRCyNiGXl+RpgU0QsBjaV52Yjo1+naiuAdWV6HXBBn7ZjNhC9CE4AD0h6TNJYGVtQWuQCvAIs6MF2zFqjF58A/XhEjEv6Q2CjpGfrMyMiJMXUlUrIxgDmHOF7FDZcGv/GRsR4edwL3EvVuXPPZGPC8rh3mvV+18nzqNnTdZkya6+mLXCPLF/xgaQjgU9Qde68H1hdFlsN3NdkO2Zt0/RUbQFwb9UNl8OA/4iI70t6FLhb0qXAS8BFDbdj1iqNghMRLwB/Ms34BHBOk9c2azNflZslODhmCQ6OWYKDY5bg4JglODhmCQ6OWYKDY5bg4JglODhmCQ6OWYKDY5bg4JglODhmCQ6OWYKDY5bg4JglpD8BKumDVN06J50E/CNwNPB3wM/L+DURsSFdoVkLpYMTEduApQCSZgHjVF1uPgfcGBFf60mFZi3Uq1O1c4AdEfFSj17PrNV6FZyVwPra8yslPSFpraQ5PdqGWWs0Do6k2cCngf8sQzcBJ1Odxu0Gru+w3pikzZI2/+9b72j0adZqvTjinA88HhF7ACJiT0Tsj4i3gVuoOnu+gzt52jDrRXBWUTtNm2x9W1xI1dnTbKQ0akhY2t6eC1xWG/6qpKVU32Kwc8o8s5HQtJPnr4B5U8YuaVSR2RDwOwfMEhwcswQHxyzBwTFLcHDMEhwcswQHxyzBwTFLcHDMEhwcswQHxyzBwTFLcHDMEhwcswQHxyzBwTFLaPRBNrO22PyFTx3wfNkN3+3r9ro64pQ2T3slba2NzZW0UdLz5XFOGZekr0vaXlpEndav4s0GpdtTtW8C500ZWwNsiojFwKbyHKquN4vLzxhVuyizkdJVcCLiQeDVKcMrgHVleh1wQW38tqg8DBw9pfON2dBrcnNgQUTsLtOvAAvK9HHAy7XldpWxA7ghoQ2zntwciIiQdFC//RFxM3AzwAnvP6xxcs796EcOeL7xxw81fUmzjpoccfZMnoKVx71lfBxYWFvu+DJmNjKaBOd+YHWZXg3cVxv/TLm7dgbwWu2UzmwkdHWqJmk9cDYwX9Iu4EvAV4C7JV0KvARcVBbfACwHtgNvUn1fjtlI6So4EbGqw6xzplk2gCuaFGXWdn7LjVmCg2OW4OCYJTg4ZgkOjlmCg2OW4M/j2Ejo9+dvpvIRxyzBwTFLcHDMEhwcs4SRuTngz9/YoeQjjlmCg2OW4OCYJTg4ZgkOjlnCjMHp0MXzXyQ9Wzp13ivp6DJ+oqRfS9pSfr7Rz+LNBqWbI843eWcXz43AqRHxx8BzwNW1eTsiYmn5ubw3ZZq1y4zBma6LZ0Q8EBH7ytOHqVpAmf2/0YtrnL8Fvld7vkjSTyX9UNKZnVZyJ08bZo3eOSDpWmAfcEcZ2g2cEBETkj4MfEfSKRHx+tR1e93J0+xQSh9xJH0W+Evgb0pLKCLiNxExUaYfA3YAH+hBnWatkgqOpPOAfwA+HRFv1saPkTSrTJ9E9VUfL/SiULM2mfFUrUMXz6uBw4GNkgAeLnfQzgL+SdJvgbeByyNi6teDmA29GYPToYvnrR2WvQe4p2lRZm3ndw6YJTg4ZgkOjlmCg2OW4OCYJTg4ZgkOjlmCg2OW4OCYJTg4ZgkOjlmCg2OW4OCYJTg4ZgkOjlmCg2OW4OCYJWQ7eV4nabzWsXN5bd7VkrZL2ibpk/0q3GyQsp08AW6sdezcACBpCbASOKWs8++TzTvMRkmqk+e7WAHcWdpEvQhsB05vUJ9ZKzW5xrmyNF1fK2lOGTsOeLm2zK4y9g7u5GnDLBucm4CTgaVU3TuvP9gXiIibI2JZRCw7araSZZgNRio4EbEnIvZHxNvALfz+dGwcWFhb9PgyZjZSsp08j609vRCYvON2P7BS0uGSFlF18vxJsxLN2ifbyfNsSUuBAHYClwFExFOS7gaepmrGfkVE7O9P6WaD09NOnmX5LwNfblKUWdv5nQNmCQ6OWYKDY5bg4JglODhmCQ6OWYKDY5bg4JglODhmCQ6OWYKDY5bg4JglODhmCQ6OWYKDY5bg4JglZBsS3lVrRrhT0pYyfqKkX9fmfaOfxZsNyoyfAKVqSPivwG2TAxHx15PTkq4HXqstvyMilvaqQLM26uaj0w9KOnG6eZIEXAT8eW/LMmu3ptc4ZwJ7IuL52tgiST+V9ENJZzZ8fbNW6uZU7d2sAtbXnu8GToiICUkfBr4j6ZSIeH3qipLGgDGAOUf4HoUNl/RvrKTDgL8C7pocKz2jJ8r0Y8AO4APTre9OnjbMmvxX/xfAsxGxa3JA0jGT304g6SSqhoQvNCvRrH26uR29HngI+KCkXZIuLbNWcuBpGsBZwBPl9vR/AZdHRLffdGA2NLINCYmIz04zdg9wT/OyzNrNV+VmCQ6OWYKDY5bg4JglODhmCQ6OWYKDY5bg4JglODhmCQ6OWYKDY5bg4JglODhmCQ6OWULTj073xNxFp3Lx7ZsGXYbZAa6aP7/jPB9xzBIcHLOEbj46vVDSDyQ9LekpSZ8v43MlbZT0fHmcU8Yl6euStkt6QtJp/d4Js0OtmyPOPuCLEbEEOAO4QtISYA2wKSIWA5vKc4DzqZp0LKZq/3RTz6s2G7AZgxMRuyPi8TL9BvAMcBywAlhXFlsHXFCmVwC3ReVh4GhJx/a8crMBOqhrnNIK90PAI8CCiNhdZr0CLCjTxwEv11bbVcbMRkbXwZF0FFUHm6umduaMiADiYDYsaUzSZkmbJyYmDmZVs4HrKjiS3ksVmjsi4ttleM/kKVh53FvGx4GFtdWPL2MHqHfynDdvXrZ+s4Ho5q6agFuBZyLihtqs+4HVZXo1cF9t/DPl7toZwGu1UzqzkdDNOwc+BlwCPDn5BVLANcBXgLtLZ8+XqL7uA2ADsBzYDrwJfK6nFZu1QDedPH8EdOqKfs40ywdwRcO6zFrN7xwwS3BwzBIcHLMEB8cswcExS1B1E2zARUg/B34F/GLQtfTQfEZnf0ZpX6D7/fmjiDhmuhmtCA6ApM0RsWzQdfTKKO3PKO0L9GZ/fKpmluDgmCW0KTg3D7qAHhul/RmlfYEe7E9rrnHMhkmbjjhmQ2PgwZF0nqRtpbnHmpnXaB9JOyU9KWmLpM1lbNpmJm0kaa2kvZK21saGthlLh/25TtJ4+TfaIml5bd7VZX+2SfpkVxuJiIH9ALOAHcBJwGzgZ8CSQdaU3I+dwPwpY18F1pTpNcA/D7rOd6n/LOA0YOtM9VN9ZOR7VO+YPwN4ZND1d7k/1wF/P82yS8rv3eHAovL7OGumbQz6iHM6sD0iXoiIt4A7qZp9jIJOzUxaJyIeBF6dMjy0zVg67E8nK4A7I+I3EfEi1efITp9ppUEHZ1QaewTwgKTHJI2VsU7NTIbFKDZjubKcXq6tnTqn9mfQwRkVH4+I06h6yl0h6az6zKjOCYb29uWw11/cBJwMLAV2A9c3ebFBB6erxh5tFxHj5XEvcC/Vob5TM5Nh0agZS9tExJ6I2B8RbwO38PvTsdT+DDo4jwKLJS2SNBtYSdXsY2hIOlLS+yangU8AW+nczGRYjFQzlinXYRdS/RtBtT8rJR0uaRFVB9qfzPiCLbgDshx4jupuxrWDridR/0lUd2V+Bjw1uQ/APKrWwM8D/wPMHXSt77IP66lOX35LdY5/aaf6qe6m/Vv593oSWDbo+rvcn2+Vep8oYTm2tvy1ZX+2Aed3sw2/c8AsYdCnamZDycExS3BwzBIcHLMEB8cswcExS3BwzBIcHLOE/wNVuC6jNsiYmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "\n",
    "for i in range(1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        img = plt.imshow(env.render(mode='rgb_array'))\n",
    "        img.set_data(env.render(mode='rgb_array'))\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        action = np.argmax(ACTOR.predict(state))\n",
    "        state, reward, done, info = env.step(action)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNDGR1zCBmbYW1JLGGFeyLF",
   "collapsed_sections": [],
   "name": "A2C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
