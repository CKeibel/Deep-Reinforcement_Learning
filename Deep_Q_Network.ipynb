{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Q-Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUbmRYEeJ9v/7WSnL/GXTY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIlX3bscb_8H"
      },
      "source": [
        "# TODO:\n",
        "\n",
        "\n",
        "* Visualize RGB State to Grayscale resized State\n",
        "* action repeat Wrapper (4 Times)\n",
        "* better reward handling for different games\n",
        "* save REWARD_LIST to Disk\n",
        "* load weight\n",
        "* play game\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9xfV_VJakaI"
      },
      "source": [
        "import gym\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hxwx0hgyS9l_"
      },
      "source": [
        "# **Auswahl des Spiels**\n",
        "\n",
        "[Hier](https://gym.openai.com/envs/#atari) ist eine vollständige Liste der verfügbaren Spiele zu finden. Um ein Environment zu erstellen muss der vollständige Name des Spiels als String übergeben werden.\n",
        "\n",
        "Beispiel:\n",
        "\n",
        "```python\n",
        "game = \"MsPacman-v0\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW8iwvvZUBxy"
      },
      "source": [
        "# Hier kann das Spiel übergeben werden\n",
        "game = \"Breakout-v0\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkVT49we0eiY"
      },
      "source": [
        "# **Preprocessing**\n",
        "[Stable Baselines](https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oHRVfJL9TxH"
      },
      "source": [
        "### Fire Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxYChLgl0aQu"
      },
      "source": [
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.Wrapper.__init__(self, env) \n",
        "        self.env.reset()\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        observation, _, _, _ = self.env.step(env.unwrapped.get_action_meanings().index('FIRE'))\n",
        "\n",
        "        return observation"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiTyOTbc9Zg6"
      },
      "source": [
        "### Frame Stack Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sumd0-F4KGYe"
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "      super().__init__(env)\n",
        "      self.frames = deque(maxlen=4)\n",
        "      low = np.repeat(self.observation_space.low[np.newaxis, ...], repeats=4, axis=0)\n",
        "      high = np.repeat(self.observation_space.high[np.newaxis, ...], repeats=4, axis=0)\n",
        "      self.observation_space = gym.spaces.Box(low=low, high=high, dtype=self.observation_space.dtype)\n",
        "\n",
        "    def step(self, action):\n",
        "      obs, reward, done, info = self.env.step(action)\n",
        "      self.frames.append(obs)\n",
        "      frame_stack = np.asarray(self.frames, dtype=np.float32)\n",
        "      frame_stack = np.moveaxis(frame_stack, 0, -1).reshape(1, 84, 84, -1)\n",
        "      return frame_stack, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs = self.env.reset(**kwargs)\n",
        "        for _ in range(4):\n",
        "          self.frames.append(obs)\n",
        "        frame_stack = np.asarray(self.frames, dtype=np.float32)\n",
        "        frame_stack = np.moveaxis(frame_stack, 0, -1).reshape(1, 84, 84, -1)\n",
        "        return frame_stack"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObV-wFKD9cXf"
      },
      "source": [
        "### Resize & Grayscale Wrapper\n",
        "\n",
        "Bild nicht Grauwert -> [stackoverflow](https://stackoverflow.com/questions/51303361/color-rgb2gray-gives-none-grayscale-image-might-be-an-issue-with-jupyter-notebo)\n",
        "```python\n",
        "cmap = \"gray\" # while plotting\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xO7wz559eWT"
      },
      "source": [
        "from gym import spaces\n",
        "import cv2\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"\n",
        "        Warp frames to 84x84 as done in the Nature paper and later work.\n",
        "        :param env: (Gym Environment) the environment\n",
        "        \"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width = 84\n",
        "        self.height = 84\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.height, self.width, 1),\n",
        "                                            dtype=env.observation_space.dtype)\n",
        "        \n",
        "    def observation(self, frame):\n",
        "        \"\"\"\n",
        "        returns the current observation from a frame\n",
        "        :param frame: ([int] or [float]) environment frame\n",
        "        :return: ([int] or [float]) the observation\n",
        "        \"\"\"\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "        return frame[:, :, None]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icOrwqqHgz7d"
      },
      "source": [
        "def make_env(env_name):\n",
        "  env = gym.make(env_name)\n",
        "  env = WarpFrame(env)\n",
        "  if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "    env = FireResetEnv(env)\n",
        "  env = FrameStack(env)\n",
        "  return env"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3Hqv2hC-_Cx",
        "outputId": "01ae7758-aacf-4818-b3bd-0086ce689e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Environment erstellen und auf Startzustand zurücksetzen\n",
        "env = make_env(game)\n",
        "env.reset()\n",
        "\n",
        "for _ in range(1, 5):\n",
        "  # Führe eine zufällige Aktion aus\n",
        "  state, _, _, _ = env.step(env.action_space.sample())\n",
        "\n",
        "# Stack umformen, damit das Plotten der vier Bilder gelingt\n",
        "state = state.reshape(84, 84,4)\n",
        "\n",
        "# Frame Stack plotten\n",
        "fig, axs = plt.subplots(1,4, figsize=(20, 5))\n",
        "fig.suptitle('Frame Stack', fontsize=20)\n",
        "for i in range(state.shape[2]):\n",
        "  axs[i].imshow(state[:, :, i], cmap=\"gray\")\n",
        "  axs[i].set_title(\"Frame \"+str(i+1), fontsize=16)\n",
        "plt.show()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8HSZBWkHkbJ"
      },
      "source": [
        "# **Deep Q-Network und Target Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeLvxPIvLxE8"
      },
      "source": [
        " *\\\"The input to\n",
        "the neural network consists of an 84x84x4 image produced by the preprocessing\n",
        "map **--w-- (falsche Darstellung)**. The first hidden layer convolves 32 filters of 8x8 with stride 4 with the\n",
        "input image and applies a rectifier nonlinearity. The second hidden layer convolves\n",
        "64 filters of 4x4 with stride 2, again followed by a rectifier nonlinearity.\n",
        "This is followed by a third convolutional layer that convolves 64 filters of 333 with\n",
        "stride 1 followed by a rectifier. The final hidden layer is fully-connected and consists\n",
        "of 512 rectifier units. The output layer is a fully-connected linear layer with a\n",
        "single output for each valid action. The number of valid actions varied between 4\n",
        "and 18 on the games we considered.\"*\n",
        "\n",
        "\n",
        "[Mnih, V., Kavukcuoglu, K., Silver, D. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015).](https://www.nature.com/articles/nature14236)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IinnQRAzHhcb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# DQN und Tagret Net Parameters\n",
        "\n",
        "INPUT_SHAPE = (84, 84, 4) # (Höhe, Breite, Frames)\n",
        "OUTPUT_SHAPE = env.action_space.n # Anzahl der möglichen Aktionen\n",
        "LOSS_FUNCTION = Huber()\n",
        "OPTIMIZER = RMSprop(learning_rate=0.00025, rho=0.95, epsilon=0.01)\n",
        "\n",
        "# Funktion zum erstellen eines neuronalen Netzes\n",
        "def build_neural_net(INPUT_SHAPE, OUTPUT_SHAPE, LOSS_FUNCTION, OPTIMIZER):\n",
        "  net_input = Input(shape=INPUT_SHAPE)\n",
        "  x = Conv2D(filters=32, kernel_size=(8, 8), strides=(4, 4), padding=\"same\")(net_input)\n",
        "  x = Activation(\"relu\")(x)\n",
        "  x = Conv2D(filters=64, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")(x)\n",
        "  x = Activation(\"relu\")(x)\n",
        "  x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
        "  x = Activation(\"relu\")(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(256)(x)\n",
        "  x = Activation(\"relu\")(x)\n",
        "  net_output = Dense(OUTPUT_SHAPE)(x)\n",
        "\n",
        "  model = Model(inputs=net_input, outputs=net_output)\n",
        "  model.compile(loss=LOSS_FUNCTION, optimizer=OPTIMIZER)\n",
        "\n",
        "  return model\n",
        "\n",
        "# Deep Q-Network\n",
        "DQN = build_neural_net(INPUT_SHAPE, OUTPUT_SHAPE, LOSS_FUNCTION, OPTIMIZER)\n",
        "# Target Network\n",
        "TARGET = build_neural_net(INPUT_SHAPE, OUTPUT_SHAPE, LOSS_FUNCTION, OPTIMIZER)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VobCYZ0N45l",
        "outputId": "e2f27e32-0c64-42d8-c182-f235d6fa9a47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "DQN.summary(), TARGET.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 84, 84, 4)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 21, 21, 32)        8224      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 21, 21, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        32832     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               1982720   \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 1028      \n",
            "=================================================================\n",
            "Total params: 2,061,732\n",
            "Trainable params: 2,061,732\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 84, 84, 4)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 21, 21, 32)        8224      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 21, 21, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 64)        32832     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               1982720   \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4)                 1028      \n",
            "=================================================================\n",
            "Total params: 2,061,732\n",
            "Trainable params: 2,061,732\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGwUt_ERhMuQ"
      },
      "source": [
        "# **Memory Buffer und Experience Replay**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z601UsvHjzyz"
      },
      "source": [
        "MEMORY_SIZE = 1000000\n",
        "MEMORY_BUFFER = deque(maxlen=MEMORY_SIZE)\n",
        "TRAIN_START = 50000\n",
        "\n",
        "# Speichert Transition von einem State in einen Folgestate\n",
        "def save_transition(state, action, reward, next_state, done):\n",
        "  MEMORY_BUFFER.append([state, action, reward, next_state, done])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v5SDTFMoZM2"
      },
      "source": [
        "import random\n",
        "MINIBATCH_SIZE = 32\n",
        "GAMMA = 0.99\n",
        "\n",
        "def replay():\n",
        "\n",
        "  # Ziehe 32 (MINIBATCH_SIZE) zufällige Transitionen aus dem Buffer in einen Minibatch\n",
        "  minibatch = random.sample(MEMORY_BUFFER, MINIBATCH_SIZE)\n",
        "  states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "  states = np.concatenate(states)\n",
        "  next_states = np.concatenate(next_states)\n",
        "\n",
        "  q_values = DQN.predict(states)\n",
        "  q_values_next = TARGET.predict(next_states)\n",
        "\n",
        "  # Q-Values Update\n",
        "  for i in range(MINIBATCH_SIZE):\n",
        "    a = actions[i]\n",
        "    done = dones[i]\n",
        "    if done:\n",
        "      q_values[i][a] = rewards[i]\n",
        "    else:\n",
        "      q_values[i][a] = rewards[i] + GAMMA * np.max(q_values_next[i])\n",
        "\n",
        "  DQN.fit(states, q_values, verbose=0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2JdsqGQf_wn"
      },
      "source": [
        "# **Aktion wählen**\n",
        "Exploration & Exploitation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH18a8XDf5ui"
      },
      "source": [
        "EPSILON = 1.0\n",
        "EPSILON_MIN = 0.01\n",
        "EPSILON_STEPS = 1000000\n",
        "\n",
        "def get_action(state):\n",
        "  if np.random.rand() <= EPSILON:\n",
        "    return np.random.randint(env.action_space.n)\n",
        "  else:\n",
        "    return np.argmax(DQN(state))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJQNx-7JhrWw"
      },
      "source": [
        "# **Training des Agenten**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BujPrlhAhqhc",
        "outputId": "2e193504-68ee-4e72-8248-a64593f946c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPISODES = 30000\n",
        "REWARD_LIST = []\n",
        "BEST_MEAN_REWARD = 0.0\n",
        "SYNC = 10000\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "  EPISODE_REWARD = 0.0\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    action = get_action(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "    # Transition im MEMORY BUFFER speichern\n",
        "    save_transition(state, action, reward, next_state, done)\n",
        "    \n",
        "    # Experience Replay, falls genügend Transitionen gespeichert\n",
        "    if len(MEMORY_BUFFER) > TRAIN_START:\n",
        "      replay()\n",
        "\n",
        "    # Reward einer Aktion zum gesamten Reward der Episode addieren\n",
        "    EPISODE_REWARD += reward\n",
        "\n",
        "    # EPSILON verringern\n",
        "    if len(MEMORY_BUFFER) > TRAIN_START:\n",
        "      if EPSILON > EPSILON_MIN:\n",
        "        EPSILON = EPSILON - ((EPSILON - EPSILON_MIN) / EPSILON_STEPS)\n",
        "\n",
        "    # State aktualisieren\n",
        "    state = next_state\n",
        "\n",
        "    # Synchronisation zwischen Target Networks und Deep Q-Network\n",
        "    if episode+1 % SYNC == 0:\n",
        "      TARGET.set_weights(DQN.get_weights())\n",
        "\n",
        "    if done:\n",
        "      REWARD_LIST.append(EPISODE_REWARD)\n",
        "      current_mean_reward = np.mean(REWARD_LIST[-min(len(REWARD_LIST), 10):])\n",
        "      print(\"Episode:\", episode+1, \"\\tReward:\", EPISODE_REWARD, \"\\tMean:\", round(current_mean_reward, 2), \"\\tTRAIN START:\", (len(MEMORY_BUFFER)>TRAIN_START), \"\\tEpsi:\", EPSILON)\n",
        "\n",
        "      # Übernahme des höchsteb Mean Rewards\n",
        "      if current_mean_reward > BEST_MEAN_REWARD:\n",
        "        BEST_MEAN_REWARD = current_mean_reward\n",
        "        \n",
        "        # Trainierte Gewichte speichern\n",
        "        if EPSILON < EPSILON_MIN:\n",
        "          import os\n",
        "          PATH = \"WEIGHTS\"\n",
        "          try:\n",
        "            os.makedirs(PATH)\n",
        "          except FileExistsError:\n",
        "            # Pfad existiert bereits\n",
        "            pass\n",
        "          DQN.save_weights(PATH +\"/\" + game + \"_DQN\"+ \"_Ep_\"+ str(episode+1)+ \".h5\")\n",
        "          TARGET.save_weights(PATH+\"/\" + game + \"_TARGET\" + \"_Ep_\"+ str(episode+1)+ \".h5\")\n",
        "      "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 1 \tReward: 3.0 \tMean: 3.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 2 \tReward: 6.0 \tMean: 4.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 3 \tReward: 2.0 \tMean: 3.67 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 4 \tReward: 3.0 \tMean: 3.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 5 \tReward: 4.0 \tMean: 3.6 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 6 \tReward: 4.0 \tMean: 3.67 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 7 \tReward: 1.0 \tMean: 3.29 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 8 \tReward: 0.0 \tMean: 2.88 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 9 \tReward: 2.0 \tMean: 2.78 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 10 \tReward: 4.0 \tMean: 2.9 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 11 \tReward: 1.0 \tMean: 2.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 12 \tReward: 2.0 \tMean: 2.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 13 \tReward: 0.0 \tMean: 2.1 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 14 \tReward: 0.0 \tMean: 1.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 15 \tReward: 0.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 16 \tReward: 0.0 \tMean: 1.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 17 \tReward: 0.0 \tMean: 0.9 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 18 \tReward: 0.0 \tMean: 0.9 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 19 \tReward: 0.0 \tMean: 0.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 20 \tReward: 2.0 \tMean: 0.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 21 \tReward: 3.0 \tMean: 0.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 22 \tReward: 1.0 \tMean: 0.6 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 23 \tReward: 1.0 \tMean: 0.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 24 \tReward: 0.0 \tMean: 0.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 25 \tReward: 0.0 \tMean: 0.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 26 \tReward: 0.0 \tMean: 0.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 27 \tReward: 1.0 \tMean: 0.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 28 \tReward: 0.0 \tMean: 0.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 29 \tReward: 0.0 \tMean: 0.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 30 \tReward: 1.0 \tMean: 0.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 31 \tReward: 1.0 \tMean: 0.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 32 \tReward: 0.0 \tMean: 0.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 33 \tReward: 0.0 \tMean: 0.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 34 \tReward: 3.0 \tMean: 0.6 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 35 \tReward: 0.0 \tMean: 0.6 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 36 \tReward: 1.0 \tMean: 0.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 37 \tReward: 2.0 \tMean: 0.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 38 \tReward: 1.0 \tMean: 0.9 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 39 \tReward: 0.0 \tMean: 0.9 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 40 \tReward: 2.0 \tMean: 1.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 41 \tReward: 0.0 \tMean: 0.9 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 42 \tReward: 2.0 \tMean: 1.1 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 43 \tReward: 0.0 \tMean: 1.1 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 44 \tReward: 0.0 \tMean: 0.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 45 \tReward: 0.0 \tMean: 0.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 46 \tReward: 1.0 \tMean: 0.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 47 \tReward: 0.0 \tMean: 0.6 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 48 \tReward: 3.0 \tMean: 0.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 49 \tReward: 2.0 \tMean: 1.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 50 \tReward: 0.0 \tMean: 0.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 51 \tReward: 1.0 \tMean: 0.9 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 52 \tReward: 1.0 \tMean: 0.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 53 \tReward: 2.0 \tMean: 1.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 54 \tReward: 0.0 \tMean: 1.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 55 \tReward: 3.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 56 \tReward: 0.0 \tMean: 1.2 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 57 \tReward: 4.0 \tMean: 1.6 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 58 \tReward: 1.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 59 \tReward: 0.0 \tMean: 1.2 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 60 \tReward: 2.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 61 \tReward: 2.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 62 \tReward: 1.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 63 \tReward: 4.0 \tMean: 1.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 64 \tReward: 0.0 \tMean: 1.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 65 \tReward: 1.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 66 \tReward: 0.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 67 \tReward: 2.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 68 \tReward: 1.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 69 \tReward: 2.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 70 \tReward: 2.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 71 \tReward: 2.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 72 \tReward: 0.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 73 \tReward: 4.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 74 \tReward: 0.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 75 \tReward: 2.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 76 \tReward: 2.0 \tMean: 1.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 77 \tReward: 1.0 \tMean: 1.6 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 78 \tReward: 1.0 \tMean: 1.6 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 79 \tReward: 2.0 \tMean: 1.6 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 80 \tReward: 1.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 81 \tReward: 2.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 82 \tReward: 2.0 \tMean: 1.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 83 \tReward: 0.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 84 \tReward: 2.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 85 \tReward: 0.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 86 \tReward: 2.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 87 \tReward: 2.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 88 \tReward: 2.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 89 \tReward: 2.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 90 \tReward: 2.0 \tMean: 1.6 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 91 \tReward: 0.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 92 \tReward: 0.0 \tMean: 1.2 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 93 \tReward: 0.0 \tMean: 1.2 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 94 \tReward: 1.0 \tMean: 1.1 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 95 \tReward: 3.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 96 \tReward: 0.0 \tMean: 1.2 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 97 \tReward: 0.0 \tMean: 1.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 98 \tReward: 1.0 \tMean: 0.9 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 99 \tReward: 5.0 \tMean: 1.2 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 100 \tReward: 2.0 \tMean: 1.2 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 101 \tReward: 2.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 102 \tReward: 1.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 103 \tReward: 8.0 \tMean: 2.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 104 \tReward: 2.0 \tMean: 2.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 105 \tReward: 2.0 \tMean: 2.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 106 \tReward: 3.0 \tMean: 2.6 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 107 \tReward: 4.0 \tMean: 3.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 108 \tReward: 1.0 \tMean: 3.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 109 \tReward: 2.0 \tMean: 2.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 110 \tReward: 0.0 \tMean: 2.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 111 \tReward: 3.0 \tMean: 2.6 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 112 \tReward: 1.0 \tMean: 2.6 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 113 \tReward: 2.0 \tMean: 2.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 114 \tReward: 0.0 \tMean: 1.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 115 \tReward: 1.0 \tMean: 1.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 116 \tReward: 2.0 \tMean: 1.6 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 117 \tReward: 2.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 118 \tReward: 0.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 119 \tReward: 0.0 \tMean: 1.1 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 120 \tReward: 2.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 121 \tReward: 2.0 \tMean: 1.2 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 122 \tReward: 2.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 123 \tReward: 0.0 \tMean: 1.1 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 124 \tReward: 0.0 \tMean: 1.1 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 125 \tReward: 0.0 \tMean: 1.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 126 \tReward: 0.0 \tMean: 0.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 127 \tReward: 1.0 \tMean: 0.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 128 \tReward: 0.0 \tMean: 0.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 129 \tReward: 1.0 \tMean: 0.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 130 \tReward: 3.0 \tMean: 0.9 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 131 \tReward: 2.0 \tMean: 0.9 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 132 \tReward: 3.0 \tMean: 1.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 133 \tReward: 1.0 \tMean: 1.1 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 134 \tReward: 0.0 \tMean: 1.1 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 135 \tReward: 1.0 \tMean: 1.2 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 136 \tReward: 1.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 137 \tReward: 5.0 \tMean: 1.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 138 \tReward: 1.0 \tMean: 1.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 139 \tReward: 2.0 \tMean: 1.9 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 140 \tReward: 4.0 \tMean: 2.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 141 \tReward: 3.0 \tMean: 2.1 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 142 \tReward: 2.0 \tMean: 2.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 143 \tReward: 1.0 \tMean: 2.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 144 \tReward: 1.0 \tMean: 2.1 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 145 \tReward: 4.0 \tMean: 2.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 146 \tReward: 1.0 \tMean: 2.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 147 \tReward: 2.0 \tMean: 2.1 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 148 \tReward: 2.0 \tMean: 2.2 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 149 \tReward: 2.0 \tMean: 2.2 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 150 \tReward: 0.0 \tMean: 1.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 151 \tReward: 0.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 152 \tReward: 0.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 153 \tReward: 2.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 154 \tReward: 1.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 155 \tReward: 1.0 \tMean: 1.1 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 156 \tReward: 0.0 \tMean: 1.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 157 \tReward: 1.0 \tMean: 0.9 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 158 \tReward: 3.0 \tMean: 1.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 159 \tReward: 5.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 160 \tReward: 1.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 161 \tReward: 3.0 \tMean: 1.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 162 \tReward: 0.0 \tMean: 1.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 163 \tReward: 0.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 164 \tReward: 0.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 165 \tReward: 4.0 \tMean: 1.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 166 \tReward: 0.0 \tMean: 1.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 167 \tReward: 1.0 \tMean: 1.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 168 \tReward: 0.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 169 \tReward: 3.0 \tMean: 1.2 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 170 \tReward: 4.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 171 \tReward: 1.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 172 \tReward: 0.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 173 \tReward: 1.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 174 \tReward: 1.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 175 \tReward: 2.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 176 \tReward: 0.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 177 \tReward: 3.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 178 \tReward: 0.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 179 \tReward: 2.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 180 \tReward: 1.0 \tMean: 1.1 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 181 \tReward: 3.0 \tMean: 1.3 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 182 \tReward: 5.0 \tMean: 1.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 183 \tReward: 2.0 \tMean: 1.9 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 184 \tReward: 2.0 \tMean: 2.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 185 \tReward: 0.0 \tMean: 1.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 186 \tReward: 2.0 \tMean: 2.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 187 \tReward: 1.0 \tMean: 1.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 188 \tReward: 2.0 \tMean: 2.0 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 189 \tReward: 0.0 \tMean: 1.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 190 \tReward: 2.0 \tMean: 1.9 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 191 \tReward: 2.0 \tMean: 1.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 192 \tReward: 2.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 193 \tReward: 2.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 194 \tReward: 1.0 \tMean: 1.4 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 195 \tReward: 3.0 \tMean: 1.7 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 196 \tReward: 0.0 \tMean: 1.5 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 197 \tReward: 2.0 \tMean: 1.6 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 198 \tReward: 2.0 \tMean: 1.6 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 199 \tReward: 2.0 \tMean: 1.8 \tTRAIN START: False \tEpsi: 1.0\n",
            "Episode: 200 \tReward: 1.0 \tMean: 1.7 \tTRAIN START: True \tEpsi: 0.999935652059157\n",
            "Episode: 201 \tReward: 1.0 \tMean: 1.6 \tTRAIN START: True \tEpsi: 0.9997357051527853\n",
            "Episode: 202 \tReward: 0.0 \tMean: 1.4 \tTRAIN START: True \tEpsi: 0.9995625164722411\n",
            "Episode: 203 \tReward: 0.0 \tMean: 1.2 \tTRAIN START: True \tEpsi: 0.9993824323923496\n",
            "Episode: 204 \tReward: 2.0 \tMean: 1.3 \tTRAIN START: True \tEpsi: 0.9990757712199134\n",
            "Episode: 205 \tReward: 1.0 \tMean: 1.1 \tTRAIN START: True \tEpsi: 0.9988443544505573\n",
            "Episode: 206 \tReward: 1.0 \tMean: 1.2 \tTRAIN START: True \tEpsi: 0.9986347415623588\n",
            "Episode: 207 \tReward: 0.0 \tMean: 1.0 \tTRAIN START: True \tEpsi: 0.9984686647935771\n",
            "Episode: 208 \tReward: 1.0 \tMean: 0.9 \tTRAIN START: True \tEpsi: 0.9982196019487545\n",
            "Episode: 209 \tReward: 2.0 \tMean: 0.9 \tTRAIN START: True \tEpsi: 0.9979251562347402\n",
            "Episode: 210 \tReward: 2.0 \tMean: 1.0 \tTRAIN START: True \tEpsi: 0.9976288229919122\n",
            "Episode: 211 \tReward: 3.0 \tMean: 1.2 \tTRAIN START: True \tEpsi: 0.9972674168033602\n",
            "Episode: 212 \tReward: 1.0 \tMean: 1.3 \tTRAIN START: True \tEpsi: 0.9970324749998738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2cb6bdef8876>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Experience Replay, falls genügend Transitionen gespeichert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMEMORY_BUFFER\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mTRAIN_START\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Reward einer Aktion zum gesamten Reward der Episode addieren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-6f8b5b1562df>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mq_values_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTARGET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1577\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1725\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m     \"\"\"\n\u001b[0;32m-> 1727\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m   def interleave(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   4121\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4122\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 4123\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   4124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4125\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3369\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3370\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3371\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2937\u001b[0m     \"\"\"\n\u001b[1;32m   2938\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 2939\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2941\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3362\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3363\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3364\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3365\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3297\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3299\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3300\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3301\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_whitelisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_batch_indices\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m    347\u001b[0m       \u001b[0mfirst_k_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_in_full_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m       first_k_indices = array_ops.reshape(\n\u001b[0;32m--> 349\u001b[0;31m           first_k_indices, [num_full_batches, batch_size])\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m       \u001b[0mflat_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_k_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m   \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   8232\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8233\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m-> 8234\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   8235\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8236\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    742\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[1;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    591\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    592\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3483\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3484\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3485\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3486\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3487\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0;32m-> 1975\u001b[0;31m                                 control_input_ops, op_def)\n\u001b[0m\u001b[1;32m   1976\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1783\u001b[0m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct_sequence_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1785\u001b[0;31m   op_desc = pywrap_tf_session.TF_NewOperation(graph._c_graph,\n\u001b[0m\u001b[1;32m   1786\u001b[0m                                               \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m                                               compat.as_str(node_def.name))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}